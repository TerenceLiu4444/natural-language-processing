{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/train.tsv is already downloaded.\n",
      "File data/validation.tsv is already downloaded.\n",
      "File data/test.tsv is already downloaded.\n",
      "File data/test_embeddings.tsv is already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "download_week3_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. You need to download it by following this [link](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing).\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    './GoogleNews-vectors-negative300.bin',\n",
    "    binary=True)\n",
    "######### YOUR CODE HERE #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above. This function should work with the input text as is without any preprocessing.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    emb_list = []\n",
    "    for word in question.split():\n",
    "        if word in embeddings:\n",
    "            #print(word)\n",
    "            emb_list.append(embeddings[word])\n",
    "            \n",
    "    if len(emb_list):\n",
    "        return np.mean(emb_list,axis=0)\n",
    "    return np.zeros(dim)\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('thereisnosuchword word', wv_embeddings)).any():\n",
    "        return \"You should not consider words which embeddings are unknown.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from the file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import array_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Question2Vec is: 0.01929389126598835\n",
      "-0.02872721292078495\n",
      "0.0460561104118824\n",
      "0.0852593332529068\n",
      "0.0243055559694767\n",
      "-0...\n"
     ]
    }
   ],
   "source": [
    "question2vec_result = []\n",
    "for question in open('data/test_embeddings.tsv'):\n",
    "    question = question.strip()\n",
    "    answer = question_to_vec(question, wv_embeddings)\n",
    "    question2vec_result = np.append(question2vec_result, answer)\n",
    "\n",
    "grader.submit_tag('Question2Vec', array_to_string(question2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, $topK(q_i)$ is the top K elements of the ranked sentences provided by our model and the operation $[dup_i \\in topK(q_i)]$ equals 1 if the condition is true and 0 otherwise (more details about this operation could be found [here](https://en.wikipedia.org/wiki/Iverson_bracket)).\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. In this case $N$ = 1 and the correct candidate for $q_1$ is *\"How does the catch keyword determine the type of exception that was thrown\"*. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] $\\text{Hits@1} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top1(q_i)] = [dup_1 \\in top1(q_1)] = 0$ because the correct answer doesn't appear in the *top1* list.\n",
    "- [K = 2] $\\text{Hits@2} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top2(q_i)] = [dup_1 \\in top2(q_1)] = 1$ because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{Hits@4} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top4(q_i)] = [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = \\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = 0$ because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] $\\text{DCG@2} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 2] = \\frac{1}{\\log_2{3}}$, because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 4] = \\frac{1}{\\log_2{3}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. Each function has two arguments: *dup_ranks* and *k*. *dup_ranks* is a list which contains *values of ranks* of duplicates. For example, *dup_ranks* is *[2]* for the example provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in Hits@k metric)\n",
    "\n",
    "        result: return Hits@k value for current ranking\n",
    "    \"\"\"\n",
    "    return 1/len(dup_ranks)*np.sum([x<=k for x in dup_ranks])\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in DCG@k metric)\n",
    "\n",
    "        result: return DCG@k value for current ranking\n",
    "    \"\"\"\n",
    "    return 1/len(dup_ranks)*np.sum([(x<=k)/np.log2(1+x) for x in dup_ranks])\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    [1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [9, 5, 4, 2, 8, 10, 7, 6, 1, 3],\n",
    "    [4, 3, 5, 1, 9, 10, 7, 8, 2, 6],\n",
    "    [5, 1, 7, 6, 2, 3, 8, 9, 10, 4],\n",
    "    [6, 3, 1, 4, 7, 2, 9, 8, 10, 5],\n",
    "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task HitsCount is: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.30000000000000004\n",
      "0.4\n",
      "0.5\n",
      "0....\n"
     ]
    }
   ],
   "source": [
    "hits_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        hits_results.append(hits_count(example, k + 1))\n",
    "grader.submit_tag('HitsCount', array_to_string(hits_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task DCGScore is: 1.0\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.3333333333333333\n",
      "0.5436432511904858\n",
      "0.7103099178...\n"
     ]
    }
   ],
   "source": [
    "dcg_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        dcg_results.append(dcg_score(example, k + 1))\n",
    "grader.submit_tag('DCGScore', array_to_string(dcg_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should upload *validation* corpus to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = read_corpus('data/validation.tsv')######### YOUR CODE HERE #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_candidates*. The function should return a sorted list of pairs *(initial position in candidates list, candidate)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity. It's preferable to use a vectorized version of *cosine_similarity* function. Try to compute similarity at once and not use list comprehension. It should speed up your computations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    v1 = question_to_vec(question, embeddings, dim)\n",
    "    v2 = np.array([question_to_vec(x, embeddings, dim) for x in candidates])\n",
    "    sim = cosine_similarity(v1.reshape(1,-1),v2)\n",
    "    return [(x, candidates[x]) for x in np.argsort(-np.squeeze(sim))]\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    for question, q_candidates, result in zip(questions, candidates, results):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "        if not np.all(ranks == result):\n",
    "            return \"Check the function.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_rank_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:34<00:00, 36.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "wv_ranking = []\n",
    "for line in tqdm(validation):\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.217 | Hits@   1: 0.217\n",
      "DCG@   5: 0.271 | Hits@   5: 0.319\n",
      "DCG@  10: 0.288 | Hits@  10: 0.370\n",
      "DCG@ 100: 0.326 | Hits@ 100: 0.560\n",
      "DCG@ 500: 0.358 | Hits@ 500: 0.818\n",
      "DCG@1000: 0.378 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to print a binary heap tree without recursion? How do you best convert a recursive function to an iterative one? How can i use ng-model with directive in angular js flash: drawing and erasing\n",
      "How to start PhoneStateListener programmatically? PhoneStateListener and service Java cast object[] to model WCF and What does this mean?\n",
      "jQuery: Show a div2 when mousenter over div1 is over when hover on div1 depenting on if it is on div2 or not it should act differently How to run selenium in google app engine/cloud? Python Comparing two lists of strings for similarities\n"
     ]
    }
   ],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:26<00:00, 380.75it/s]\n"
     ]
    }
   ],
   "source": [
    "prepared_validation = []\n",
    "for line in tqdm(validation):\n",
    "    prepared_validation.append([text_prepare(x) for x in line])\n",
    "    ######### YOUR CODE HERE #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:59<00:00, 41.84it/s]\n"
     ]
    }
   ],
   "source": [
    "wv_prepared_ranking = []\n",
    "for line in tqdm(prepared_validation):\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.320 | Hits@   1: 0.320\n",
      "DCG@   5: 0.387 | Hits@   5: 0.448\n",
      "DCG@  10: 0.404 | Hits@  10: 0.500\n",
      "DCG@ 100: 0.438 | Hits@ 100: 0.669\n",
      "DCG@ 500: 0.460 | Hits@ 500: 0.844\n",
      "DCG@1000: 0.477 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_file('data/train.tsv','data/train_prepared.tsv')\n",
    "prepare_file('data/test.tsv','data/test_prepared.tsv')\n",
    "\n",
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. The calculations should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task W2VTokenizedRanks is: 95\t94\t7\t9\t64\t37\t32\t93\t24\t100\t98\t17\t60\t6\t97\t49\t70\t38\t42\t96\t30\t21\t2\t65\t67\t45\t27\t26\t57\t62\t11\t88\t56\t66\t7...\n"
     ]
    }
   ],
   "source": [
    "w2v_ranks_results = []\n",
    "prepared_test_data = 'data/test_prepared.tsv' ######### YOUR CODE HERE #############\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace cannot be run on Windows and we recommend to use provided\n",
    "[docker container](https://github.com/hse-aml/natural-language-processing/blob/master/Docker-tutorial.md) or other alternatives. Don't delete results of this task because you will need it in the final project.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training mode. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a number of negative examples which is used during the training. We think that 10 will be enought for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/train_prepared.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  95058\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/train_prepared.tsv\n",
      "Total number of examples loaded : 999740\n",
      "Initialized model weights. Model size :\n",
      "matrix : 95058 100\n",
      "Training epoch 0: 0.05 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64.1%  lr: 0.043414  loss: 0.010584  eta: 0h4m  tot: 0h0m40s  (12.8%)%  lr: 0.050000  loss: 0.078742  eta: 0h5m  tot: 0h0m0s  (0.1%)1.1%  lr: 0.049940  loss: 0.051382  eta: 0h5m  tot: 0h0m0s  (0.2%)%  lr: 0.049930  loss: 0.048859  eta: 0h5m  tot: 0h0m0s  (0.3%)1.4%  lr: 0.049920  loss: 0.047498  eta: 0h5m  tot: 0h0m0s  (0.3%)2.2%  lr: 0.049840  loss: 0.040855  eta: 0h5m  tot: 0h0m1s  (0.4%)%  lr: 0.049840  loss: 0.040457  eta: 0h5m  tot: 0h0m1s  (0.5%)2.4%  lr: 0.049830  loss: 0.039841  eta: 0h5m  tot: 0h0m1s  (0.5%)3.5%  lr: 0.049720  loss: 0.034075  eta: 0h5m  tot: 0h0m2s  (0.7%)3.9%  lr: 0.049610  loss: 0.032561  eta: 0h5m  tot: 0h0m2s  (0.8%)4.9%  lr: 0.049510  loss: 0.030091  eta: 0h5m  tot: 0h0m3s  (1.0%)5.1%  lr: 0.049439  loss: 0.029476  eta: 0h5m  tot: 0h0m3s  (1.0%)5.3%  lr: 0.049419  loss: 0.028986  eta: 0h5m  tot: 0h0m3s  (1.1%)%  lr: 0.049399  loss: 0.027870  eta: 0h5m  tot: 0h0m4s  (1.1%)6.2%  lr: 0.049329  loss: 0.026935  eta: 0h5m  tot: 0h0m4s  (1.2%)%  lr: 0.049279  loss: 0.026222  eta: 0h5m  tot: 0h0m4s  (1.3%)6.8%  lr: 0.049259  loss: 0.025656  eta: 0h5m  tot: 0h0m4s  (1.4%)6.9%  lr: 0.049239  loss: 0.025512  eta: 0h5m  tot: 0h0m4s  (1.4%)7.1%  lr: 0.049229  loss: 0.025255  eta: 0h5m  tot: 0h0m5s  (1.4%)7.5%  lr: 0.049169  loss: 0.024830  eta: 0h5m  tot: 0h0m5s  (1.5%)7.6%  lr: 0.049169  loss: 0.024739  eta: 0h5m  tot: 0h0m5s  (1.5%)8.2%  lr: 0.049079  loss: 0.023971  eta: 0h5m  tot: 0h0m5s  (1.6%)8.5%  lr: 0.049019  loss: 0.023510  eta: 0h5m  tot: 0h0m6s  (1.7%)8.9%  lr: 0.048989  loss: 0.022931  eta: 0h6m  tot: 0h0m6s  (1.8%)%  lr: 0.048929  loss: 0.022507  eta: 0h6m  tot: 0h0m6s  (1.9%)9.7%  lr: 0.048869  loss: 0.022053  eta: 0h5m  tot: 0h0m7s  (1.9%)10.2%  lr: 0.048819  loss: 0.021536  eta: 0h5m  tot: 0h0m7s  (2.0%)10.4%  lr: 0.048809  loss: 0.021321  eta: 0h5m  tot: 0h0m7s  (2.1%)11.0%  lr: 0.048789  loss: 0.020915  eta: 0h5m  tot: 0h0m7s  (2.2%)12.3%  lr: 0.048649  loss: 0.020085  eta: 0h5m  tot: 0h0m8s  (2.5%)12.6%  lr: 0.048629  loss: 0.019897  eta: 0h5m  tot: 0h0m8s  (2.5%)12.9%  lr: 0.048619  loss: 0.019803  eta: 0h5m  tot: 0h0m9s  (2.6%)13.3%  lr: 0.048579  loss: 0.019594  eta: 0h5m  tot: 0h0m9s  (2.7%)14.6%  lr: 0.048458  loss: 0.018844  eta: 0h5m  tot: 0h0m10s  (2.9%)14.9%  lr: 0.048438  loss: 0.018627  eta: 0h5m  tot: 0h0m10s  (3.0%)15.6%  lr: 0.048358  loss: 0.018350  eta: 0h5m  tot: 0h0m10s  (3.1%)16.6%  lr: 0.048228  loss: 0.017844  eta: 0h5m  tot: 0h0m11s  (3.3%)18.5%  lr: 0.048098  loss: 0.017057  eta: 0h5m  tot: 0h0m12s  (3.7%)19.4%  lr: 0.048028  loss: 0.016785  eta: 0h5m  tot: 0h0m12s  (3.9%)19.6%  lr: 0.047998  loss: 0.016703  eta: 0h5m  tot: 0h0m12s  (3.9%)20.1%  lr: 0.047968  loss: 0.016575  eta: 0h5m  tot: 0h0m13s  (4.0%)21.6%  lr: 0.047778  loss: 0.016064  eta: 0h5m  tot: 0h0m14s  (4.3%)22.0%  lr: 0.047738  loss: 0.015942  eta: 0h5m  tot: 0h0m14s  (4.4%)%  lr: 0.047728  loss: 0.015926  eta: 0h5m  tot: 0h0m14s  (4.4%)22.5%  lr: 0.047678  loss: 0.015785  eta: 0h5m  tot: 0h0m14s  (4.5%)22.6%  lr: 0.047678  loss: 0.015767  eta: 0h5m  tot: 0h0m14s  (4.5%)%  lr: 0.047668  loss: 0.015734  eta: 0h5m  tot: 0h0m15s  (4.6%)23.2%  lr: 0.047588  loss: 0.015635  eta: 0h5m  tot: 0h0m15s  (4.6%)23.4%  lr: 0.047558  loss: 0.015589  eta: 0h5m  tot: 0h0m15s  (4.7%)24.6%  lr: 0.047447  loss: 0.015361  eta: 0h5m  tot: 0h0m16s  (4.9%)24.7%  lr: 0.047437  loss: 0.015347  eta: 0h5m  tot: 0h0m16s  (4.9%)24.8%  lr: 0.047437  loss: 0.015296  eta: 0h5m  tot: 0h0m16s  (5.0%)%  lr: 0.047407  loss: 0.015206  eta: 0h5m  tot: 0h0m16s  (5.0%)25.4%  lr: 0.047397  loss: 0.015153  eta: 0h5m  tot: 0h0m16s  (5.1%)26.1%  lr: 0.047277  loss: 0.015003  eta: 0h5m  tot: 0h0m17s  (5.2%)26.6%  lr: 0.047227  loss: 0.014909  eta: 0h5m  tot: 0h0m17s  (5.3%)%  lr: 0.047197  loss: 0.014777  eta: 0h5m  tot: 0h0m18s  (5.5%)28.0%  lr: 0.047137  loss: 0.014618  eta: 0h5m  tot: 0h0m18s  (5.6%)28.3%  lr: 0.047107  loss: 0.014553  eta: 0h5m  tot: 0h0m18s  (5.7%)28.7%  lr: 0.047087  loss: 0.014491  eta: 0h5m  tot: 0h0m18s  (5.7%)%  lr: 0.047017  loss: 0.014276  eta: 0h5m  tot: 0h0m19s  (6.0%)31.2%  lr: 0.046927  loss: 0.014088  eta: 0h5m  tot: 0h0m20s  (6.2%)31.7%  lr: 0.046877  loss: 0.014005  eta: 0h5m  tot: 0h0m20s  (6.3%)32.0%  lr: 0.046847  loss: 0.013934  eta: 0h5m  tot: 0h0m21s  (6.4%)33.1%  lr: 0.046737  loss: 0.013765  eta: 0h5m  tot: 0h0m21s  (6.6%)33.8%  lr: 0.046657  loss: 0.013678  eta: 0h5m  tot: 0h0m22s  (6.8%)34.4%  lr: 0.046577  loss: 0.013603  eta: 0h5m  tot: 0h0m22s  (6.9%)34.5%  lr: 0.046567  loss: 0.013579  eta: 0h5m  tot: 0h0m22s  (6.9%)34.7%  lr: 0.046527  loss: 0.013523  eta: 0h5m  tot: 0h0m22s  (6.9%)36.5%  lr: 0.046296  loss: 0.013254  eta: 0h5m  tot: 0h0m23s  (7.3%)37.2%  lr: 0.046266  loss: 0.013166  eta: 0h5m  tot: 0h0m24s  (7.4%)37.4%  lr: 0.046236  loss: 0.013149  eta: 0h5m  tot: 0h0m24s  (7.5%)38.9%  lr: 0.046146  loss: 0.012968  eta: 0h4m  tot: 0h0m25s  (7.8%)39.3%  lr: 0.046046  loss: 0.012900  eta: 0h4m  tot: 0h0m25s  (7.9%)%  lr: 0.046006  loss: 0.012857  eta: 0h5m  tot: 0h0m25s  (7.9%)40.0%  lr: 0.045976  loss: 0.012813  eta: 0h4m  tot: 0h0m26s  (8.0%)40.1%  lr: 0.045936  loss: 0.012792  eta: 0h5m  tot: 0h0m26s  (8.0%)40.3%  lr: 0.045926  loss: 0.012758  eta: 0h5m  tot: 0h0m26s  (8.1%)41.0%  lr: 0.045816  loss: 0.012639  eta: 0h5m  tot: 0h0m26s  (8.2%)41.6%  lr: 0.045776  loss: 0.012554  eta: 0h5m  tot: 0h0m27s  (8.3%)42.8%  lr: 0.045686  loss: 0.012427  eta: 0h4m  tot: 0h0m27s  (8.6%)43.3%  lr: 0.045636  loss: 0.012394  eta: 0h4m  tot: 0h0m28s  (8.7%)44.1%  lr: 0.045576  loss: 0.012302  eta: 0h4m  tot: 0h0m28s  (8.8%)44.2%  lr: 0.045556  loss: 0.012288  eta: 0h4m  tot: 0h0m28s  (8.8%)45.4%  lr: 0.045456  loss: 0.012149  eta: 0h4m  tot: 0h0m29s  (9.1%)46.1%  lr: 0.045355  loss: 0.012078  eta: 0h4m  tot: 0h0m30s  (9.2%)%  lr: 0.045285  loss: 0.012024  eta: 0h4m  tot: 0h0m30s  (9.3%)46.7%  lr: 0.045265  loss: 0.012018  eta: 0h4m  tot: 0h0m30s  (9.3%)47.2%  lr: 0.045165  loss: 0.011977  eta: 0h4m  tot: 0h0m30s  (9.4%)47.7%  lr: 0.045115  loss: 0.011926  eta: 0h4m  tot: 0h0m31s  (9.5%)49.1%  lr: 0.045035  loss: 0.011787  eta: 0h4m  tot: 0h0m32s  (9.8%)49.2%  lr: 0.045005  loss: 0.011782  eta: 0h4m  tot: 0h0m32s  (9.8%)49.3%  lr: 0.044995  loss: 0.011762  eta: 0h4m  tot: 0h0m32s  (9.9%)49.5%  lr: 0.044955  loss: 0.011741  eta: 0h4m  tot: 0h0m32s  (9.9%)%  lr: 0.044885  loss: 0.011677  eta: 0h4m  tot: 0h0m32s  (10.0%)50.7%  lr: 0.044835  loss: 0.011619  eta: 0h4m  tot: 0h0m33s  (10.1%)50.8%  lr: 0.044815  loss: 0.011608  eta: 0h4m  tot: 0h0m33s  (10.2%)50.9%  lr: 0.044815  loss: 0.011599  eta: 0h4m  tot: 0h0m33s  (10.2%)51.6%  lr: 0.044745  loss: 0.011536  eta: 0h4m  tot: 0h0m33s  (10.3%)51.9%  lr: 0.044705  loss: 0.011502  eta: 0h4m  tot: 0h0m33s  (10.4%)52.3%  lr: 0.044695  loss: 0.011482  eta: 0h4m  tot: 0h0m34s  (10.5%)52.6%  lr: 0.044655  loss: 0.011463  eta: 0h4m  tot: 0h0m34s  (10.5%)52.7%  lr: 0.044655  loss: 0.011460  eta: 0h4m  tot: 0h0m34s  (10.5%)%  lr: 0.044605  loss: 0.011438  eta: 0h4m  tot: 0h0m34s  (10.6%)53.0%  lr: 0.044595  loss: 0.011425  eta: 0h4m  tot: 0h0m34s  (10.6%)53.1%  lr: 0.044575  loss: 0.011415  eta: 0h4m  tot: 0h0m34s  (10.6%)53.7%  lr: 0.044495  loss: 0.011358  eta: 0h4m  tot: 0h0m35s  (10.7%)53.9%  lr: 0.044475  loss: 0.011338  eta: 0h4m  tot: 0h0m35s  (10.8%)54.8%  lr: 0.044425  loss: 0.011259  eta: 0h4m  tot: 0h0m35s  (11.0%)55.3%  lr: 0.044404  loss: 0.011235  eta: 0h4m  tot: 0h0m35s  (11.1%)55.5%  lr: 0.044374  loss: 0.011223  eta: 0h4m  tot: 0h0m36s  (11.1%)56.1%  lr: 0.044344  loss: 0.011171  eta: 0h4m  tot: 0h0m36s  (11.2%)58.5%  lr: 0.044054  loss: 0.011027  eta: 0h4m  tot: 0h0m37s  (11.7%)58.8%  lr: 0.044034  loss: 0.011002  eta: 0h4m  tot: 0h0m37s  (11.8%)59.1%  lr: 0.043954  loss: 0.010976  eta: 0h4m  tot: 0h0m38s  (11.8%)59.3%  lr: 0.043914  loss: 0.010958  eta: 0h4m  tot: 0h0m38s  (11.9%)59.5%  lr: 0.043894  loss: 0.010944  eta: 0h4m  tot: 0h0m38s  (11.9%)60.4%  lr: 0.043764  loss: 0.010868  eta: 0h4m  tot: 0h0m38s  (12.1%)62.2%  lr: 0.043634  loss: 0.010734  eta: 0h4m  tot: 0h0m39s  (12.4%)63.2%  lr: 0.043524  loss: 0.010652  eta: 0h4m  tot: 0h0m40s  (12.6%)63.6%  lr: 0.043464  loss: 0.010622  eta: 0h4m  tot: 0h0m40s  (12.7%)64.0%  lr: 0.043414  loss: 0.010593  eta: 0h4m  tot: 0h0m40s  (12.8%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.040000  loss: 0.008905  eta: h4m  tot: 0h1m1s  (20.0%))64.9%  lr: 0.043353  loss: 0.010533  eta: 0h4m  tot: 0h0m41s  (13.0%)65.2%  lr: 0.043323  loss: 0.010515  eta: 0h4m  tot: 0h0m41s  (13.0%)65.9%  lr: 0.043243  loss: 0.010475  eta: 0h4m  tot: 0h0m41s  (13.2%)66.7%  lr: 0.043173  loss: 0.010417  eta: 0h4m  tot: 0h0m42s  (13.3%)67.2%  lr: 0.043123  loss: 0.010387  eta: 0h4m  tot: 0h0m42s  (13.4%)69.1%  lr: 0.043013  loss: 0.010282  eta: 0h4m  tot: 0h0m43s  (13.8%)69.9%  lr: 0.042963  loss: 0.010239  eta: 0h4m  tot: 0h0m43s  (14.0%)70.9%  lr: 0.042873  loss: 0.010188  eta: 0h4m  tot: 0h0m44s  (14.2%)71.2%  lr: 0.042853  loss: 0.010167  eta: 0h4m  tot: 0h0m44s  (14.2%)%  lr: 0.042733  loss: 0.010111  eta: 0h4m  tot: 0h0m45s  (14.5%)72.9%  lr: 0.042683  loss: 0.010071  eta: 0h4m  tot: 0h0m45s  (14.6%)73.2%  lr: 0.042643  loss: 0.010060  eta: 0h4m  tot: 0h0m45s  (14.6%)%  lr: 0.042633  loss: 0.010041  eta: 0h4m  tot: 0h0m45s  (14.7%)75.2%  lr: 0.042493  loss: 0.009959  eta: 0h4m  tot: 0h0m46s  (15.0%)75.8%  lr: 0.042423  loss: 0.009921  eta: 0h4m  tot: 0h0m47s  (15.2%)%  lr: 0.042413  loss: 0.009906  eta: 0h4m  tot: 0h0m47s  (15.2%)76.8%  lr: 0.042362  loss: 0.009866  eta: 0h4m  tot: 0h0m47s  (15.4%)78.5%  lr: 0.042162  loss: 0.009789  eta: 0h4m  tot: 0h0m48s  (15.7%)78.7%  lr: 0.042142  loss: 0.009781  eta: 0h4m  tot: 0h0m48s  (15.7%)80.4%  lr: 0.041922  loss: 0.009700  eta: 0h4m  tot: 0h0m49s  (16.1%)80.5%  lr: 0.041912  loss: 0.009699  eta: 0h4m  tot: 0h0m49s  (16.1%)%  lr: 0.041892  loss: 0.009692  eta: 0h4m  tot: 0h0m50s  (16.1%)80.8%  lr: 0.041892  loss: 0.009691  eta: 0h4m  tot: 0h0m50s  (16.2%)%  lr: 0.041832  loss: 0.009662  eta: 0h4m  tot: 0h0m50s  (16.3%)81.8%  lr: 0.041812  loss: 0.009648  eta: 0h4m  tot: 0h0m50s  (16.4%)82.6%  lr: 0.041722  loss: 0.009610  eta: 0h4m  tot: 0h0m51s  (16.5%)82.8%  lr: 0.041692  loss: 0.009595  eta: 0h4m  tot: 0h0m51s  (16.6%)82.9%  lr: 0.041662  loss: 0.009591  eta: 0h4m  tot: 0h0m51s  (16.6%)83.3%  lr: 0.041582  loss: 0.009585  eta: 0h4m  tot: 0h0m51s  (16.7%)%  lr: 0.041351  loss: 0.009521  eta: 0h4m  tot: 0h0m52s  (16.9%)85.6%  lr: 0.041131  loss: 0.009483  eta: 0h4m  tot: 0h0m53s  (17.1%)86.2%  lr: 0.041051  loss: 0.009456  eta: 0h4m  tot: 0h0m54s  (17.2%)87.1%  lr: 0.040961  loss: 0.009417  eta: 0h4m  tot: 0h0m54s  (17.4%)88.9%  lr: 0.040741  loss: 0.009346  eta: 0h4m  tot: 0h0m55s  (17.8%)90.0%  lr: 0.040631  loss: 0.009289  eta: 0h4m  tot: 0h0m56s  (18.0%)90.3%  lr: 0.040601  loss: 0.009284  eta: 0h4m  tot: 0h0m56s  (18.1%)90.6%  lr: 0.040591  loss: 0.009276  eta: 0h4m  tot: 0h0m56s  (18.1%)90.9%  lr: 0.040531  loss: 0.009272  eta: 0h4m  tot: 0h0m56s  (18.2%)91.5%  lr: 0.040521  loss: 0.009248  eta: 0h4m  tot: 0h0m57s  (18.3%)91.6%  lr: 0.040511  loss: 0.009244  eta: 0h4m  tot: 0h0m57s  (18.3%)91.8%  lr: 0.040511  loss: 0.009234  eta: 0h4m  tot: 0h0m57s  (18.4%)92.1%  lr: 0.040481  loss: 0.009233  eta: 0h4m  tot: 0h0m57s  (18.4%)%  lr: 0.040280  loss: 0.009153  eta: 0h4m  tot: 0h0m58s  (18.8%)94.9%  lr: 0.040220  loss: 0.009121  eta: 0h4m  tot: 0h0m59s  (19.0%)95.7%  lr: 0.040170  loss: 0.009081  eta: 0h4m  tot: 0h0m59s  (19.1%)97.3%  lr: 0.040040  loss: 0.009013  eta: 0h4m  tot: 0h1m0s  (19.5%)  loss: 0.008988  eta: 0h4m  tot: 0h1m0s  (19.6%)0h4m  tot: 0h1m1s  (20.0%)\n",
      " ---+++                Epoch    0 Train error : 0.00895454 +++--- ☃\n",
      "Training epoch 1: 0.04 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74.5%  lr: 0.032122  loss: 0.002696  eta: 0h3m  tot: 0h1m46s  (34.9%)3%  lr: 0.039830  loss: 0.002015  eta: 0h4m  tot: 0h1m1s  (20.3%)1.9%  lr: 0.039810  loss: 0.002334  eta: 0h3m  tot: 0h1m2s  (20.4%)2.0%  lr: 0.039790  loss: 0.002318  eta: 0h3m  tot: 0h1m2s  (20.4%)2.3%  lr: 0.039770  loss: 0.002429  eta: 0h3m  tot: 0h1m2s  (20.5%)3.2%  lr: 0.039700  loss: 0.002460  eta: 0h3m  tot: 0h1m2s  (20.6%)3.3%  lr: 0.039690  loss: 0.002461  eta: 0h3m  tot: 0h1m2s  (20.7%)4.4%  lr: 0.039550  loss: 0.002456  eta: 0h3m  tot: 0h1m3s  (20.9%)4.5%  lr: 0.039550  loss: 0.002445  eta: 0h3m  tot: 0h1m3s  (20.9%)4.8%  lr: 0.039540  loss: 0.002399  eta: 0h3m  tot: 0h1m3s  (21.0%)7.7%  lr: 0.039289  loss: 0.002508  eta: 0h3m  tot: 0h1m5s  (21.5%)8.6%  lr: 0.039219  loss: 0.002520  eta: 0h3m  tot: 0h1m5s  (21.7%)9.0%  lr: 0.039169  loss: 0.002537  eta: 0h3m  tot: 0h1m6s  (21.8%)9.5%  lr: 0.039129  loss: 0.002529  eta: 0h3m  tot: 0h1m6s  (21.9%)11.8%  lr: 0.038899  loss: 0.002545  eta: 0h3m  tot: 0h1m7s  (22.4%)12.1%  lr: 0.038879  loss: 0.002537  eta: 0h3m  tot: 0h1m7s  (22.4%)%  lr: 0.038799  loss: 0.002586  eta: 0h3m  tot: 0h1m8s  (22.5%)13.0%  lr: 0.038759  loss: 0.002578  eta: 0h3m  tot: 0h1m8s  (22.6%)%  lr: 0.038709  loss: 0.002582  eta: 0h3m  tot: 0h1m8s  (22.7%)14.5%  lr: 0.038599  loss: 0.002604  eta: 0h3m  tot: 0h1m9s  (22.9%)15.8%  lr: 0.038468  loss: 0.002614  eta: 0h3m  tot: 0h1m10s  (23.2%)15.8%  lr: 0.038458  loss: 0.002612  eta: 0h3m  tot: 0h1m10s  (23.2%)16.1%  lr: 0.038398  loss: 0.002609  eta: 0h3m  tot: 0h1m10s  (23.2%)16.7%  lr: 0.038348  loss: 0.002616  eta: 0h3m  tot: 0h1m10s  (23.3%)17.0%  lr: 0.038308  loss: 0.002609  eta: 0h3m  tot: 0h1m10s  (23.4%)18.4%  lr: 0.038158  loss: 0.002591  eta: 0h3m  tot: 0h1m11s  (23.7%)18.7%  lr: 0.038148  loss: 0.002634  eta: 0h3m  tot: 0h1m12s  (23.7%)19.0%  lr: 0.038138  loss: 0.002642  eta: 0h3m  tot: 0h1m12s  (23.8%)19.3%  lr: 0.038128  loss: 0.002636  eta: 0h3m  tot: 0h1m12s  (23.9%)20.5%  lr: 0.038078  loss: 0.002601  eta: 0h3m  tot: 0h1m12s  (24.1%)20.9%  lr: 0.038008  loss: 0.002603  eta: 0h3m  tot: 0h1m13s  (24.2%)21.5%  lr: 0.037858  loss: 0.002596  eta: 0h3m  tot: 0h1m13s  (24.3%)%  lr: 0.037848  loss: 0.002591  eta: 0h3m  tot: 0h1m13s  (24.3%)21.8%  lr: 0.037838  loss: 0.002583  eta: 0h3m  tot: 0h1m13s  (24.4%)22.7%  lr: 0.037788  loss: 0.002597  eta: 0h3m  tot: 0h1m14s  (24.5%)22.9%  lr: 0.037778  loss: 0.002598  eta: 0h3m  tot: 0h1m14s  (24.6%)23.0%  lr: 0.037728  loss: 0.002603  eta: 0h3m  tot: 0h1m14s  (24.6%)23.2%  lr: 0.037698  loss: 0.002598  eta: 0h3m  tot: 0h1m14s  (24.6%)23.6%  lr: 0.037648  loss: 0.002598  eta: 0h3m  tot: 0h1m15s  (24.7%)23.8%  lr: 0.037638  loss: 0.002591  eta: 0h3m  tot: 0h1m15s  (24.8%)%  lr: 0.037498  loss: 0.002617  eta: 0h3m  tot: 0h1m16s  (25.0%)25.1%  lr: 0.037427  loss: 0.002618  eta: 0h3m  tot: 0h1m16s  (25.0%)26.5%  lr: 0.037307  loss: 0.002609  eta: 0h3m  tot: 0h1m17s  (25.3%)27.7%  lr: 0.037177  loss: 0.002608  eta: 0h3m  tot: 0h1m17s  (25.5%)28.2%  lr: 0.037127  loss: 0.002631  eta: 0h3m  tot: 0h1m17s  (25.6%)29.4%  lr: 0.037007  loss: 0.002649  eta: 0h3m  tot: 0h1m18s  (25.9%)30.0%  lr: 0.036987  loss: 0.002640  eta: 0h3m  tot: 0h1m18s  (26.0%)%  lr: 0.036907  loss: 0.002642  eta: 0h3m  tot: 0h1m19s  (26.1%)30.9%  lr: 0.036877  loss: 0.002642  eta: 0h3m  tot: 0h1m19s  (26.2%)32.5%  lr: 0.036697  loss: 0.002658  eta: 0h3m  tot: 0h1m20s  (26.5%)32.9%  lr: 0.036647  loss: 0.002653  eta: 0h3m  tot: 0h1m20s  (26.6%)33.8%  lr: 0.036477  loss: 0.002677  eta: 0h3m  tot: 0h1m21s  (26.8%)34.3%  lr: 0.036406  loss: 0.002675  eta: 0h3m  tot: 0h1m21s  (26.9%)34.8%  lr: 0.036366  loss: 0.002677  eta: 0h3m  tot: 0h1m21s  (27.0%)35.3%  lr: 0.036306  loss: 0.002677  eta: 0h3m  tot: 0h1m22s  (27.1%)36.1%  lr: 0.036196  loss: 0.002669  eta: 0h3m  tot: 0h1m22s  (27.2%)36.6%  lr: 0.036096  loss: 0.002675  eta: 0h3m  tot: 0h1m22s  (27.3%)36.9%  lr: 0.036086  loss: 0.002667  eta: 0h3m  tot: 0h1m23s  (27.4%)37.1%  lr: 0.036086  loss: 0.002671  eta: 0h3m  tot: 0h1m23s  (27.4%)38.2%  lr: 0.035986  loss: 0.002674  eta: 0h3m  tot: 0h1m23s  (27.6%)38.4%  lr: 0.035966  loss: 0.002672  eta: 0h3m  tot: 0h1m24s  (27.7%)%  lr: 0.035906  loss: 0.002666  eta: 0h3m  tot: 0h1m24s  (27.7%)%  lr: 0.035886  loss: 0.002660  eta: 0h3m  tot: 0h1m24s  (27.8%)39.4%  lr: 0.035816  loss: 0.002673  eta: 0h3m  tot: 0h1m24s  (27.9%)39.5%  lr: 0.035796  loss: 0.002670  eta: 0h3m  tot: 0h1m24s  (27.9%)39.7%  lr: 0.035756  loss: 0.002664  eta: 0h3m  tot: 0h1m25s  (27.9%)40.8%  lr: 0.035706  loss: 0.002680  eta: 0h3m  tot: 0h1m25s  (28.2%)41.0%  lr: 0.035696  loss: 0.002691  eta: 0h3m  tot: 0h1m25s  (28.2%)41.9%  lr: 0.035616  loss: 0.002697  eta: 0h3m  tot: 0h1m26s  (28.4%)42.1%  lr: 0.035576  loss: 0.002706  eta: 0h3m  tot: 0h1m26s  (28.4%)42.4%  lr: 0.035546  loss: 0.002713  eta: 0h3m  tot: 0h1m26s  (28.5%)%  lr: 0.035425  loss: 0.002710  eta: 0h3m  tot: 0h1m27s  (28.8%)44.3%  lr: 0.035365  loss: 0.002720  eta: 0h3m  tot: 0h1m28s  (28.9%)44.4%  lr: 0.035335  loss: 0.002719  eta: 0h3m  tot: 0h1m28s  (28.9%)44.7%  lr: 0.035295  loss: 0.002719  eta: 0h3m  tot: 0h1m28s  (28.9%)%  lr: 0.035275  loss: 0.002719  eta: 0h3m  tot: 0h1m28s  (29.0%)46.3%  lr: 0.035145  loss: 0.002719  eta: 0h3m  tot: 0h1m29s  (29.3%)46.5%  lr: 0.035105  loss: 0.002720  eta: 0h3m  tot: 0h1m29s  (29.3%)46.6%  lr: 0.035095  loss: 0.002721  eta: 0h3m  tot: 0h1m29s  (29.3%)47.8%  lr: 0.034905  loss: 0.002726  eta: 0h3m  tot: 0h1m30s  (29.6%)49.0%  lr: 0.034835  loss: 0.002728  eta: 0h3m  tot: 0h1m30s  (29.8%)50.4%  lr: 0.034715  loss: 0.002721  eta: 0h3m  tot: 0h1m31s  (30.1%)51.5%  lr: 0.034605  loss: 0.002709  eta: 0h3m  tot: 0h1m31s  (30.3%)52.1%  lr: 0.034525  loss: 0.002707  eta: 0h3m  tot: 0h1m32s  (30.4%)%  lr: 0.034515  loss: 0.002707  eta: 0h3m  tot: 0h1m32s  (30.5%)52.9%  lr: 0.034455  loss: 0.002709  eta: 0h3m  tot: 0h1m32s  (30.6%)53.0%  lr: 0.034425  loss: 0.002712  eta: 0h3m  tot: 0h1m33s  (30.6%)54.5%  lr: 0.034304  loss: 0.002713  eta: 0h3m  tot: 0h1m33s  (30.9%)54.7%  lr: 0.034284  loss: 0.002712  eta: 0h3m  tot: 0h1m33s  (30.9%)55.1%  lr: 0.034264  loss: 0.002713  eta: 0h3m  tot: 0h1m34s  (31.0%)55.5%  lr: 0.034214  loss: 0.002705  eta: 0h3m  tot: 0h1m34s  (31.1%)56.0%  lr: 0.034154  loss: 0.002705  eta: 0h3m  tot: 0h1m34s  (31.2%)56.1%  lr: 0.034134  loss: 0.002702  eta: 0h3m  tot: 0h1m34s  (31.2%)%  lr: 0.034064  loss: 0.002705  eta: 0h3m  tot: 0h1m35s  (31.4%)%  lr: 0.034034  loss: 0.002702  eta: 0h3m  tot: 0h1m35s  (31.4%)57.4%  lr: 0.033994  loss: 0.002706  eta: 0h3m  tot: 0h1m35s  (31.5%)57.6%  lr: 0.033974  loss: 0.002707  eta: 0h3m  tot: 0h1m35s  (31.5%)58.0%  lr: 0.033954  loss: 0.002706  eta: 0h3m  tot: 0h1m36s  (31.6%)%  lr: 0.033874  loss: 0.002699  eta: 0h3m  tot: 0h1m36s  (31.7%)60.9%  lr: 0.033634  loss: 0.002708  eta: 0h3m  tot: 0h1m37s  (32.2%)61.4%  lr: 0.033574  loss: 0.002712  eta: 0h3m  tot: 0h1m38s  (32.3%)%  lr: 0.033544  loss: 0.002710  eta: 0h3m  tot: 0h1m38s  (32.3%)61.8%  lr: 0.033534  loss: 0.002707  eta: 0h3m  tot: 0h1m38s  (32.4%)62.8%  lr: 0.033343  loss: 0.002706  eta: 0h3m  tot: 0h1m39s  (32.6%)63.0%  lr: 0.033323  loss: 0.002705  eta: 0h3m  tot: 0h1m39s  (32.6%)63.6%  lr: 0.033263  loss: 0.002706  eta: 0h3m  tot: 0h1m39s  (32.7%)63.8%  lr: 0.033253  loss: 0.002703  eta: 0h3m  tot: 0h1m39s  (32.8%)65.3%  lr: 0.033073  loss: 0.002698  eta: 0h3m  tot: 0h1m40s  (33.1%)66.2%  lr: 0.033023  loss: 0.002699  eta: 0h3m  tot: 0h1m41s  (33.2%)66.3%  lr: 0.033023  loss: 0.002697  eta: 0h3m  tot: 0h1m41s  (33.3%)%  lr: 0.032953  loss: 0.002699  eta: 0h3m  tot: 0h1m41s  (33.3%)67.2%  lr: 0.032853  loss: 0.002700  eta: 0h3m  tot: 0h1m41s  (33.4%)67.4%  lr: 0.032823  loss: 0.002699  eta: 0h3m  tot: 0h1m42s  (33.5%)%  lr: 0.032783  loss: 0.002699  eta: 0h3m  tot: 0h1m42s  (33.5%)68.4%  lr: 0.032703  loss: 0.002709  eta: 0h3m  tot: 0h1m42s  (33.7%)69.0%  lr: 0.032643  loss: 0.002710  eta: 0h3m  tot: 0h1m43s  (33.8%)70.5%  lr: 0.032513  loss: 0.002706  eta: 0h3m  tot: 0h1m43s  (34.1%)70.8%  lr: 0.032473  loss: 0.002705  eta: 0h3m  tot: 0h1m44s  (34.2%)72.0%  lr: 0.032403  loss: 0.002698  eta: 0h3m  tot: 0h1m44s  (34.4%)73.8%  lr: 0.032182  loss: 0.002696  eta: 0h3m  tot: 0h1m45s  (34.8%)74.6%  lr: 0.032112  loss: 0.002695  eta: 0h3m  tot: 0h1m46s  (34.9%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.030000  loss: 0.002647  eta: 0h2m  tot: 0h1m59s  (40.0%)5.3%  lr: 0.032032  loss: 0.002687  eta: 0h3m  tot: 0h1m46s  (35.1%)75.6%  lr: 0.032002  loss: 0.002688  eta: 0h3m  tot: 0h1m46s  (35.1%)76.2%  lr: 0.031972  loss: 0.002685  eta: 0h3m  tot: 0h1m46s  (35.2%)76.4%  lr: 0.031942  loss: 0.002684  eta: 0h3m  tot: 0h1m47s  (35.3%)77.2%  lr: 0.031872  loss: 0.002679  eta: 0h3m  tot: 0h1m47s  (35.4%)77.4%  lr: 0.031872  loss: 0.002680  eta: 0h3m  tot: 0h1m47s  (35.5%)77.7%  lr: 0.031832  loss: 0.002675  eta: 0h3m  tot: 0h1m47s  (35.5%)78.1%  lr: 0.031742  loss: 0.002677  eta: 0h3m  tot: 0h1m48s  (35.6%)78.3%  lr: 0.031712  loss: 0.002678  eta: 0h3m  tot: 0h1m48s  (35.7%)78.6%  lr: 0.031682  loss: 0.002676  eta: 0h3m  tot: 0h1m48s  (35.7%)79.6%  lr: 0.031562  loss: 0.002678  eta: 0h3m  tot: 0h1m49s  (35.9%)79.9%  lr: 0.031542  loss: 0.002679  eta: 0h3m  tot: 0h1m49s  (36.0%)80.6%  lr: 0.031502  loss: 0.002674  eta: 0h3m  tot: 0h1m49s  (36.1%)%  lr: 0.031502  loss: 0.002677  eta: 0h3m  tot: 0h1m49s  (36.2%)80.8%  lr: 0.031502  loss: 0.002678  eta: 0h3m  tot: 0h1m49s  (36.2%)82.0%  lr: 0.031422  loss: 0.002675  eta: 0h3m  tot: 0h1m50s  (36.4%)82.7%  lr: 0.031311  loss: 0.002674  eta: 0h3m  tot: 0h1m51s  (36.5%)82.8%  lr: 0.031311  loss: 0.002672  eta: 0h3m  tot: 0h1m51s  (36.6%)83.5%  lr: 0.031191  loss: 0.002673  eta: 0h3m  tot: 0h1m51s  (36.7%)84.4%  lr: 0.031151  loss: 0.002671  eta: 0h3m  tot: 0h1m51s  (36.9%)84.9%  lr: 0.031091  loss: 0.002674  eta: 0h3m  tot: 0h1m52s  (37.0%)85.2%  lr: 0.031071  loss: 0.002671  eta: 0h3m  tot: 0h1m52s  (37.0%)85.4%  lr: 0.031061  loss: 0.002668  eta: 0h3m  tot: 0h1m52s  (37.1%)%  lr: 0.030921  loss: 0.002665  eta: 0h3m  tot: 0h1m52s  (37.2%)86.6%  lr: 0.030881  loss: 0.002663  eta: 0h3m  tot: 0h1m53s  (37.3%)%  lr: 0.030831  loss: 0.002666  eta: 0h3m  tot: 0h1m53s  (37.4%)87.3%  lr: 0.030821  loss: 0.002666  eta: 0h3m  tot: 0h1m53s  (37.5%)88.2%  lr: 0.030711  loss: 0.002665  eta: 0h3m  tot: 0h1m54s  (37.6%)%  lr: 0.030571  loss: 0.002664  eta: 0h3m  tot: 0h1m54s  (37.9%)89.4%  lr: 0.030551  loss: 0.002666  eta: 0h3m  tot: 0h1m54s  (37.9%)%  lr: 0.030511  loss: 0.002665  eta: 0h3m  tot: 0h1m55s  (38.0%)90.1%  lr: 0.030501  loss: 0.002664  eta: 0h3m  tot: 0h1m55s  (38.0%)90.6%  lr: 0.030461  loss: 0.002664  eta: 0h3m  tot: 0h1m55s  (38.1%)91.0%  lr: 0.030411  loss: 0.002664  eta: 0h3m  tot: 0h1m55s  (38.2%)91.3%  lr: 0.030391  loss: 0.002665  eta: 0h3m  tot: 0h1m56s  (38.3%)91.5%  lr: 0.030391  loss: 0.002663  eta: 0h3m  tot: 0h1m56s  (38.3%)92.3%  lr: 0.030330  loss: 0.002656  eta: 0h3m  tot: 0h1m56s  (38.5%)%  lr: 0.030270  loss: 0.002658  eta: 0h3m  tot: 0h1m56s  (38.5%)93.2%  lr: 0.030250  loss: 0.002661  eta: 0h3m  tot: 0h1m57s  (38.6%)93.4%  lr: 0.030240  loss: 0.002666  eta: 0h3m  tot: 0h1m57s  (38.7%)93.7%  lr: 0.030230  loss: 0.002665  eta: 0h3m  tot: 0h1m57s  (38.7%)94.0%  lr: 0.030210  loss: 0.002664  eta: 0h3m  tot: 0h1m57s  (38.8%)94.3%  lr: 0.030170  loss: 0.002664  eta: 0h3m  tot: 0h1m57s  (38.9%)%  lr: 0.030160  loss: 0.002664  eta: 0h3m  tot: 0h1m57s  (38.9%)95.2%  lr: 0.030100  loss: 0.002663  eta: 0h3m  tot: 0h1m58s  (39.0%)95.4%  lr: 0.030090  loss: 0.002660  eta: 0h3m  tot: 0h1m58s  (39.1%)0h3m  tot: 0h1m58s  (39.3%)0h2m  tot: 0h1m58s  (39.5%)2m  tot: 0h1m59s  (39.6%)2m  tot: 0h1m59s  (39.8%)h2m  tot: 0h1m59s  (40.0%)\n",
      " ---+++                Epoch    1 Train error : 0.00266338 +++--- ☃\n",
      "Training epoch 2: 0.03 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61.8%  lr: 0.023273  loss: 0.001915  eta: 0h2m  tot: 0h2m38s  (52.4%).3%  lr: 0.029990  loss: 0.001685  eta: 0h2m  tot: 0h1m59s  (40.1%)0.9%  lr: 0.029920  loss: 0.001808  eta: 0h2m  tot: 0h2m0s  (40.2%)1.9%  lr: 0.029830  loss: 0.001736  eta: 0h2m  tot: 0h2m0s  (40.4%)%  lr: 0.029790  loss: 0.001819  eta: 0h2m  tot: 0h2m0s  (40.4%)2.4%  lr: 0.029720  loss: 0.001861  eta: 0h3m  tot: 0h2m1s  (40.5%)2.6%  lr: 0.029720  loss: 0.001901  eta: 0h3m  tot: 0h2m1s  (40.5%)3.3%  lr: 0.029590  loss: 0.001861  eta: 0h3m  tot: 0h2m1s  (40.7%)4.0%  lr: 0.029459  loss: 0.001772  eta: 0h3m  tot: 0h2m2s  (40.8%)5.5%  lr: 0.029379  loss: 0.001874  eta: 0h3m  tot: 0h2m3s  (41.1%)5.7%  lr: 0.029359  loss: 0.001877  eta: 0h3m  tot: 0h2m3s  (41.1%)6.8%  lr: 0.029209  loss: 0.001832  eta: 0h2m  tot: 0h2m3s  (41.4%)7.2%  lr: 0.029179  loss: 0.001805  eta: 0h2m  tot: 0h2m3s  (41.4%)7.7%  lr: 0.029129  loss: 0.001773  eta: 0h2m  tot: 0h2m4s  (41.5%)8.0%  lr: 0.029089  loss: 0.001775  eta: 0h2m  tot: 0h2m4s  (41.6%)%  lr: 0.029079  loss: 0.001786  eta: 0h2m  tot: 0h2m4s  (41.6%)9.0%  lr: 0.028969  loss: 0.001799  eta: 0h2m  tot: 0h2m5s  (41.8%)9.1%  lr: 0.028969  loss: 0.001819  eta: 0h2m  tot: 0h2m5s  (41.8%)9.5%  lr: 0.028959  loss: 0.001810  eta: 0h2m  tot: 0h2m5s  (41.9%)10.2%  lr: 0.028909  loss: 0.001798  eta: 0h2m  tot: 0h2m5s  (42.0%)11.7%  lr: 0.028819  loss: 0.001772  eta: 0h2m  tot: 0h2m6s  (42.3%)%  lr: 0.028759  loss: 0.001791  eta: 0h2m  tot: 0h2m6s  (42.4%)12.6%  lr: 0.028719  loss: 0.001784  eta: 0h2m  tot: 0h2m7s  (42.5%)12.7%  lr: 0.028719  loss: 0.001787  eta: 0h2m  tot: 0h2m7s  (42.5%)12.9%  lr: 0.028719  loss: 0.001788  eta: 0h2m  tot: 0h2m7s  (42.6%)13.5%  lr: 0.028679  loss: 0.001784  eta: 0h2m  tot: 0h2m7s  (42.7%)13.6%  lr: 0.028679  loss: 0.001786  eta: 0h2m  tot: 0h2m7s  (42.7%)%  lr: 0.028649  loss: 0.001791  eta: 0h2m  tot: 0h2m7s  (42.8%)14.5%  lr: 0.028619  loss: 0.001787  eta: 0h2m  tot: 0h2m8s  (42.9%)14.8%  lr: 0.028559  loss: 0.001809  eta: 0h2m  tot: 0h2m8s  (43.0%)15.4%  lr: 0.028448  loss: 0.001826  eta: 0h2m  tot: 0h2m8s  (43.1%)15.5%  lr: 0.028428  loss: 0.001826  eta: 0h2m  tot: 0h2m8s  (43.1%)%  lr: 0.028338  loss: 0.001841  eta: 0h2m  tot: 0h2m9s  (43.2%)16.3%  lr: 0.028308  loss: 0.001846  eta: 0h2m  tot: 0h2m9s  (43.3%)16.7%  lr: 0.028298  loss: 0.001855  eta: 0h2m  tot: 0h2m9s  (43.3%)17.1%  lr: 0.028288  loss: 0.001866  eta: 0h2m  tot: 0h2m9s  (43.4%)17.3%  lr: 0.028268  loss: 0.001864  eta: 0h2m  tot: 0h2m10s  (43.5%)17.5%  lr: 0.028228  loss: 0.001863  eta: 0h2m  tot: 0h2m10s  (43.5%)17.9%  lr: 0.028198  loss: 0.001853  eta: 0h2m  tot: 0h2m10s  (43.6%)19.4%  lr: 0.028088  loss: 0.001831  eta: 0h2m  tot: 0h2m11s  (43.9%)19.9%  lr: 0.028038  loss: 0.001837  eta: 0h2m  tot: 0h2m11s  (44.0%)20.6%  lr: 0.027948  loss: 0.001822  eta: 0h2m  tot: 0h2m12s  (44.1%)20.8%  lr: 0.027948  loss: 0.001817  eta: 0h2m  tot: 0h2m12s  (44.2%)21.0%  lr: 0.027938  loss: 0.001816  eta: 0h2m  tot: 0h2m12s  (44.2%)21.2%  lr: 0.027868  loss: 0.001809  eta: 0h2m  tot: 0h2m12s  (44.2%)22.1%  lr: 0.027718  loss: 0.001830  eta: 0h2m  tot: 0h2m13s  (44.4%)23.9%  lr: 0.027568  loss: 0.001811  eta: 0h2m  tot: 0h2m14s  (44.8%)24.3%  lr: 0.027538  loss: 0.001827  eta: 0h2m  tot: 0h2m14s  (44.9%)25.2%  lr: 0.027407  loss: 0.001829  eta: 0h2m  tot: 0h2m15s  (45.0%)25.7%  lr: 0.027357  loss: 0.001835  eta: 0h2m  tot: 0h2m15s  (45.1%)26.0%  lr: 0.027297  loss: 0.001839  eta: 0h2m  tot: 0h2m15s  (45.2%)26.7%  lr: 0.027247  loss: 0.001836  eta: 0h2m  tot: 0h2m16s  (45.3%)27.1%  lr: 0.027187  loss: 0.001840  eta: 0h2m  tot: 0h2m16s  (45.4%)27.3%  lr: 0.027137  loss: 0.001835  eta: 0h2m  tot: 0h2m16s  (45.5%)28.3%  lr: 0.026997  loss: 0.001848  eta: 0h2m  tot: 0h2m17s  (45.7%)28.7%  lr: 0.026987  loss: 0.001845  eta: 0h2m  tot: 0h2m17s  (45.7%)29.7%  lr: 0.026897  loss: 0.001841  eta: 0h2m  tot: 0h2m17s  (45.9%)30.4%  lr: 0.026837  loss: 0.001844  eta: 0h2m  tot: 0h2m18s  (46.1%)32.0%  lr: 0.026717  loss: 0.001848  eta: 0h2m  tot: 0h2m19s  (46.4%)32.1%  lr: 0.026697  loss: 0.001851  eta: 0h2m  tot: 0h2m19s  (46.4%)32.9%  lr: 0.026607  loss: 0.001858  eta: 0h2m  tot: 0h2m19s  (46.6%)33.6%  lr: 0.026477  loss: 0.001864  eta: 0h2m  tot: 0h2m20s  (46.7%)34.3%  lr: 0.026336  loss: 0.001875  eta: 0h2m  tot: 0h2m20s  (46.9%)35.2%  lr: 0.026246  loss: 0.001869  eta: 0h2m  tot: 0h2m21s  (47.0%)35.9%  lr: 0.026226  loss: 0.001875  eta: 0h2m  tot: 0h2m21s  (47.2%)36.9%  lr: 0.026156  loss: 0.001884  eta: 0h2m  tot: 0h2m22s  (47.4%)37.5%  lr: 0.026116  loss: 0.001882  eta: 0h2m  tot: 0h2m22s  (47.5%)37.8%  lr: 0.026086  loss: 0.001880  eta: 0h2m  tot: 0h2m22s  (47.6%)38.2%  lr: 0.026046  loss: 0.001878  eta: 0h2m  tot: 0h2m22s  (47.6%)%  lr: 0.026026  loss: 0.001872  eta: 0h2m  tot: 0h2m22s  (47.7%)38.5%  lr: 0.026016  loss: 0.001872  eta: 0h2m  tot: 0h2m23s  (47.7%)38.7%  lr: 0.025976  loss: 0.001882  eta: 0h2m  tot: 0h2m23s  (47.7%)39.1%  lr: 0.025906  loss: 0.001879  eta: 0h2m  tot: 0h2m23s  (47.8%)%  lr: 0.025896  loss: 0.001887  eta: 0h2m  tot: 0h2m23s  (47.8%)39.3%  lr: 0.025896  loss: 0.001886  eta: 0h2m  tot: 0h2m23s  (47.9%)40.1%  lr: 0.025826  loss: 0.001892  eta: 0h2m  tot: 0h2m24s  (48.0%)40.1%  lr: 0.025816  loss: 0.001889  eta: 0h2m  tot: 0h2m24s  (48.0%)40.3%  lr: 0.025816  loss: 0.001891  eta: 0h2m  tot: 0h2m24s  (48.1%)41.1%  lr: 0.025716  loss: 0.001885  eta: 0h2m  tot: 0h2m24s  (48.2%)%  lr: 0.025616  loss: 0.001889  eta: 0h2m  tot: 0h2m25s  (48.4%)43.0%  lr: 0.025516  loss: 0.001901  eta: 0h2m  tot: 0h2m26s  (48.6%)44.2%  lr: 0.025355  loss: 0.001895  eta: 0h2m  tot: 0h2m26s  (48.8%)44.4%  lr: 0.025335  loss: 0.001896  eta: 0h2m  tot: 0h2m26s  (48.9%)44.5%  lr: 0.025285  loss: 0.001900  eta: 0h2m  tot: 0h2m26s  (48.9%)44.8%  lr: 0.025265  loss: 0.001896  eta: 0h2m  tot: 0h2m27s  (49.0%)46.9%  lr: 0.025025  loss: 0.001888  eta: 0h2m  tot: 0h2m28s  (49.4%)%  lr: 0.024965  loss: 0.001889  eta: 0h2m  tot: 0h2m28s  (49.5%)47.9%  lr: 0.024915  loss: 0.001897  eta: 0h2m  tot: 0h2m28s  (49.6%)48.5%  lr: 0.024865  loss: 0.001908  eta: 0h2m  tot: 0h2m29s  (49.7%)48.7%  lr: 0.024845  loss: 0.001909  eta: 0h2m  tot: 0h2m29s  (49.7%)48.8%  lr: 0.024845  loss: 0.001908  eta: 0h2m  tot: 0h2m29s  (49.8%)%  lr: 0.024745  loss: 0.001913  eta: 0h2m  tot: 0h2m30s  (50.0%)50.6%  lr: 0.024685  loss: 0.001913  eta: 0h2m  tot: 0h2m30s  (50.1%)50.9%  lr: 0.024665  loss: 0.001911  eta: 0h2m  tot: 0h2m30s  (50.2%)51.0%  lr: 0.024655  loss: 0.001910  eta: 0h2m  tot: 0h2m30s  (50.2%)51.6%  lr: 0.024615  loss: 0.001909  eta: 0h2m  tot: 0h2m31s  (50.3%)52.0%  lr: 0.024515  loss: 0.001906  eta: 0h2m  tot: 0h2m31s  (50.4%)52.4%  lr: 0.024435  loss: 0.001907  eta: 0h2m  tot: 0h2m31s  (50.5%)%  lr: 0.024374  loss: 0.001910  eta: 0h2m  tot: 0h2m31s  (50.5%)%  lr: 0.024364  loss: 0.001912  eta: 0h2m  tot: 0h2m32s  (50.6%)52.9%  lr: 0.024364  loss: 0.001912  eta: 0h2m  tot: 0h2m32s  (50.6%)%  lr: 0.024234  loss: 0.001917  eta: 0h2m  tot: 0h2m32s  (50.7%)54.0%  lr: 0.024194  loss: 0.001917  eta: 0h2m  tot: 0h2m32s  (50.8%)54.5%  lr: 0.024144  loss: 0.001912  eta: 0h2m  tot: 0h2m33s  (50.9%)54.9%  lr: 0.024074  loss: 0.001915  eta: 0h2m  tot: 0h2m33s  (51.0%)55.2%  lr: 0.024054  loss: 0.001920  eta: 0h2m  tot: 0h2m33s  (51.0%)55.5%  lr: 0.024004  loss: 0.001915  eta: 0h2m  tot: 0h2m34s  (51.1%)55.9%  lr: 0.023974  loss: 0.001916  eta: 0h2m  tot: 0h2m34s  (51.2%)56.6%  lr: 0.023934  loss: 0.001918  eta: 0h2m  tot: 0h2m34s  (51.3%)56.8%  lr: 0.023924  loss: 0.001919  eta: 0h2m  tot: 0h2m34s  (51.4%)57.4%  lr: 0.023814  loss: 0.001923  eta: 0h2m  tot: 0h2m35s  (51.5%)57.7%  lr: 0.023754  loss: 0.001925  eta: 0h2m  tot: 0h2m35s  (51.5%)57.9%  lr: 0.023694  loss: 0.001923  eta: 0h2m  tot: 0h2m35s  (51.6%)%  lr: 0.023694  loss: 0.001921  eta: 0h2m  tot: 0h2m35s  (51.6%)58.8%  lr: 0.023614  loss: 0.001914  eta: 0h2m  tot: 0h2m36s  (51.8%)60.2%  lr: 0.023484  loss: 0.001917  eta: 0h2m  tot: 0h2m36s  (52.0%)60.3%  lr: 0.023474  loss: 0.001916  eta: 0h2m  tot: 0h2m36s  (52.1%)60.9%  lr: 0.023404  loss: 0.001919  eta: 0h2m  tot: 0h2m37s  (52.2%)%  lr: 0.023383  loss: 0.001918  eta: 0h2m  tot: 0h2m37s  (52.2%)61.7%  lr: 0.023293  loss: 0.001917  eta: 0h2m  tot: 0h2m37s  (52.3%)61.8%  lr: 0.023293  loss: 0.001916  eta: 0h2m  tot: 0h2m37s  (52.4%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.020000  loss: 0.001919  eta: 0h1m  tot: 0h2m57s  (60.0%)3.4%  lr: 0.023183  loss: 0.001911  eta: 0h2m  tot: 0h2m38s  (52.7%)%  lr: 0.023173  loss: 0.001910  eta: 0h2m  tot: 0h2m39s  (52.8%)64.1%  lr: 0.023153  loss: 0.001907  eta: 0h2m  tot: 0h2m39s  (52.8%)64.7%  lr: 0.023073  loss: 0.001908  eta: 0h2m  tot: 0h2m39s  (52.9%)65.0%  lr: 0.023033  loss: 0.001908  eta: 0h2m  tot: 0h2m39s  (53.0%)65.6%  lr: 0.022983  loss: 0.001907  eta: 0h2m  tot: 0h2m40s  (53.1%)65.9%  lr: 0.022953  loss: 0.001910  eta: 0h2m  tot: 0h2m40s  (53.2%)%  lr: 0.022893  loss: 0.001912  eta: 0h2m  tot: 0h2m40s  (53.4%)67.6%  lr: 0.022843  loss: 0.001909  eta: 0h2m  tot: 0h2m41s  (53.5%)67.8%  lr: 0.022813  loss: 0.001909  eta: 0h2m  tot: 0h2m41s  (53.6%)68.0%  lr: 0.022773  loss: 0.001906  eta: 0h2m  tot: 0h2m41s  (53.6%)68.1%  lr: 0.022763  loss: 0.001905  eta: 0h2m  tot: 0h2m41s  (53.6%)%  lr: 0.022713  loss: 0.001906  eta: 0h2m  tot: 0h2m41s  (53.8%)69.4%  lr: 0.022693  loss: 0.001907  eta: 0h2m  tot: 0h2m42s  (53.9%)69.5%  lr: 0.022693  loss: 0.001905  eta: 0h2m  tot: 0h2m42s  (53.9%)71.3%  lr: 0.022573  loss: 0.001908  eta: 0h2m  tot: 0h2m43s  (54.3%)74.0%  lr: 0.022393  loss: 0.001910  eta: 0h2m  tot: 0h2m44s  (54.8%)75.3%  lr: 0.022202  loss: 0.001910  eta: 0h2m  tot: 0h2m45s  (55.1%)76.2%  lr: 0.022092  loss: 0.001909  eta: 0h2m  tot: 0h2m45s  (55.2%)76.5%  lr: 0.022062  loss: 0.001906  eta: 0h2m  tot: 0h2m45s  (55.3%)%  lr: 0.021992  loss: 0.001906  eta: 0h2m  tot: 0h2m46s  (55.4%)%  lr: 0.021942  loss: 0.001907  eta: 0h2m  tot: 0h2m46s  (55.5%)77.9%  lr: 0.021902  loss: 0.001904  eta: 0h2m  tot: 0h2m46s  (55.6%)78.0%  lr: 0.021882  loss: 0.001906  eta: 0h2m  tot: 0h2m46s  (55.6%)78.2%  lr: 0.021822  loss: 0.001904  eta: 0h2m  tot: 0h2m47s  (55.6%)78.5%  lr: 0.021792  loss: 0.001905  eta: 0h2m  tot: 0h2m47s  (55.7%)79.8%  lr: 0.021702  loss: 0.001906  eta: 0h2m  tot: 0h2m47s  (56.0%)79.9%  lr: 0.021682  loss: 0.001908  eta: 0h2m  tot: 0h2m47s  (56.0%)80.5%  lr: 0.021642  loss: 0.001914  eta: 0h2m  tot: 0h2m48s  (56.1%)80.7%  lr: 0.021632  loss: 0.001915  eta: 0h2m  tot: 0h2m48s  (56.1%)81.0%  lr: 0.021602  loss: 0.001914  eta: 0h2m  tot: 0h2m48s  (56.2%)81.3%  lr: 0.021562  loss: 0.001914  eta: 0h2m  tot: 0h2m48s  (56.3%)81.4%  lr: 0.021552  loss: 0.001913  eta: 0h2m  tot: 0h2m48s  (56.3%)%  lr: 0.021532  loss: 0.001912  eta: 0h2m  tot: 0h2m48s  (56.3%)81.9%  lr: 0.021442  loss: 0.001915  eta: 0h2m  tot: 0h2m49s  (56.4%)82.3%  lr: 0.021412  loss: 0.001917  eta: 0h2m  tot: 0h2m49s  (56.5%)82.7%  lr: 0.021382  loss: 0.001916  eta: 0h2m  tot: 0h2m49s  (56.5%)83.0%  lr: 0.021321  loss: 0.001914  eta: 0h2m  tot: 0h2m49s  (56.6%)84.8%  lr: 0.021231  loss: 0.001913  eta: 0h2m  tot: 0h2m50s  (57.0%)85.4%  lr: 0.021181  loss: 0.001911  eta: 0h2m  tot: 0h2m50s  (57.1%)85.7%  lr: 0.021171  loss: 0.001914  eta: 0h2m  tot: 0h2m50s  (57.1%)85.8%  lr: 0.021151  loss: 0.001918  eta: 0h2m  tot: 0h2m51s  (57.2%)86.2%  lr: 0.021111  loss: 0.001920  eta: 0h2m  tot: 0h2m51s  (57.2%)86.8%  lr: 0.021051  loss: 0.001920  eta: 0h2m  tot: 0h2m51s  (57.4%)87.0%  lr: 0.021031  loss: 0.001922  eta: 0h2m  tot: 0h2m51s  (57.4%)87.6%  lr: 0.020951  loss: 0.001921  eta: 0h2m  tot: 0h2m51s  (57.5%)%  lr: 0.020911  loss: 0.001921  eta: 0h2m  tot: 0h2m52s  (57.7%)89.0%  lr: 0.020871  loss: 0.001924  eta: 0h2m  tot: 0h2m52s  (57.8%)89.1%  lr: 0.020861  loss: 0.001924  eta: 0h2m  tot: 0h2m52s  (57.8%)89.5%  lr: 0.020841  loss: 0.001922  eta: 0h2m  tot: 0h2m53s  (57.9%)91.3%  lr: 0.020651  loss: 0.001930  eta: 0h2m  tot: 0h2m53s  (58.3%)91.8%  lr: 0.020551  loss: 0.001926  eta: 0h2m  tot: 0h2m54s  (58.4%)92.0%  lr: 0.020521  loss: 0.001928  eta: 0h2m  tot: 0h2m54s  (58.4%)92.3%  lr: 0.020491  loss: 0.001926  eta: 0h2m  tot: 0h2m54s  (58.5%)%  lr: 0.020491  loss: 0.001926  eta: 0h2m  tot: 0h2m54s  (58.5%)92.9%  lr: 0.020381  loss: 0.001925  eta: 0h2m  tot: 0h2m54s  (58.6%)%  lr: 0.020340  loss: 0.001923  eta: 0h2m  tot: 0h2m55s  (58.7%)93.9%  lr: 0.020280  loss: 0.001924  eta: 0h2m  tot: 0h2m55s  (58.8%)94.3%  lr: 0.020260  loss: 0.001922  eta: 0h2m  tot: 0h2m55s  (58.9%)95.8%  lr: 0.020100  loss: 0.001929  eta: 0h2m  tot: 0h2m56s  (59.2%)  lr: 0.020050  loss: 0.001930  eta: 0h2m  tot: 0h2m56s  (59.3%)  lr: 0.020040  loss: 0.001928  eta: 0h1m  tot: 0h2m57s  (59.5%)0h1m  tot: 0h2m57s  (59.9%)\n",
      " ---+++                Epoch    2 Train error : 0.00190441 +++--- ☃\n",
      "Training epoch 3: 0.02 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66.4%  lr: 0.012673  loss: 0.001589  eta: 0h1m  tot: 0h3m39s  (73.3%).2%  lr: 0.019990  loss: 0.000927  eta: 0h1m  tot: 0h2m58s  (60.0%)0.3%  lr: 0.019990  loss: 0.001034  eta: 0h1m  tot: 0h2m58s  (60.1%)0.4%  lr: 0.019960  loss: 0.001410  eta: 0h2m  tot: 0h2m58s  (60.1%)0.6%  lr: 0.019950  loss: 0.001632  eta: 0h2m  tot: 0h2m58s  (60.1%)%  lr: 0.019940  loss: 0.001585  eta: 0h2m  tot: 0h2m58s  (60.1%)m58s  (60.2%)1.2%  lr: 0.019890  loss: 0.001500  eta: 0h2m  tot: 0h2m58s  (60.2%)1.4%  lr: 0.019860  loss: 0.001565  eta: 0h2m  tot: 0h2m59s  (60.3%)%  lr: 0.019850  loss: 0.001574  eta: 0h2m  tot: 0h2m59s  (60.3%)3.5%  lr: 0.019720  loss: 0.001639  eta: 0h2m  tot: 0h3m0s  (60.7%)3.9%  lr: 0.019720  loss: 0.001657  eta: 0h2m  tot: 0h3m0s  (60.8%)4.1%  lr: 0.019680  loss: 0.001648  eta: 0h2m  tot: 0h3m0s  (60.8%)%  lr: 0.019550  loss: 0.001601  eta: 0h2m  tot: 0h3m1s  (61.1%)6.7%  lr: 0.019389  loss: 0.001576  eta: 0h2m  tot: 0h3m2s  (61.3%)7.9%  lr: 0.019289  loss: 0.001574  eta: 0h2m  tot: 0h3m2s  (61.6%)%  lr: 0.019179  loss: 0.001612  eta: 0h2m  tot: 0h3m3s  (61.8%)9.3%  lr: 0.019159  loss: 0.001613  eta: 0h2m  tot: 0h3m3s  (61.9%)9.5%  lr: 0.019139  loss: 0.001628  eta: 0h2m  tot: 0h3m3s  (61.9%)9.6%  lr: 0.019119  loss: 0.001619  eta: 0h2m  tot: 0h3m4s  (61.9%)11.5%  lr: 0.018859  loss: 0.001574  eta: 0h2m  tot: 0h3m5s  (62.3%)%  lr: 0.018609  loss: 0.001558  eta: 0h2m  tot: 0h3m6s  (62.5%)13.0%  lr: 0.018559  loss: 0.001547  eta: 0h2m  tot: 0h3m6s  (62.6%)%  lr: 0.018559  loss: 0.001544  eta: 0h2m  tot: 0h3m6s  (62.6%)%  lr: 0.018388  loss: 0.001533  eta: 0h2m  tot: 0h3m7s  (62.7%)14.2%  lr: 0.018338  loss: 0.001517  eta: 0h2m  tot: 0h3m7s  (62.8%)14.3%  lr: 0.018338  loss: 0.001514  eta: 0h2m  tot: 0h3m7s  (62.9%)14.4%  lr: 0.018318  loss: 0.001514  eta: 0h2m  tot: 0h3m7s  (62.9%)%  lr: 0.018018  loss: 0.001535  eta: 0h2m  tot: 0h3m9s  (63.4%)17.6%  lr: 0.017958  loss: 0.001531  eta: 0h2m  tot: 0h3m9s  (63.5%)17.8%  lr: 0.017918  loss: 0.001532  eta: 0h2m  tot: 0h3m9s  (63.6%)17.9%  lr: 0.017918  loss: 0.001531  eta: 0h2m  tot: 0h3m9s  (63.6%)19.0%  lr: 0.017828  loss: 0.001535  eta: 0h1m  tot: 0h3m10s  (63.8%)19.9%  lr: 0.017778  loss: 0.001528  eta: 0h1m  tot: 0h3m10s  (64.0%)20.0%  lr: 0.017778  loss: 0.001527  eta: 0h1m  tot: 0h3m11s  (64.0%)20.1%  lr: 0.017728  loss: 0.001529  eta: 0h1m  tot: 0h3m11s  (64.0%)%  lr: 0.017628  loss: 0.001530  eta: 0h1m  tot: 0h3m11s  (64.2%)21.5%  lr: 0.017598  loss: 0.001544  eta: 0h1m  tot: 0h3m12s  (64.3%)21.9%  lr: 0.017578  loss: 0.001541  eta: 0h1m  tot: 0h3m12s  (64.4%)22.6%  lr: 0.017498  loss: 0.001538  eta: 0h1m  tot: 0h3m12s  (64.5%)%  lr: 0.017347  loss: 0.001545  eta: 0h1m  tot: 0h3m13s  (64.7%)24.1%  lr: 0.017317  loss: 0.001542  eta: 0h1m  tot: 0h3m13s  (64.8%)%  lr: 0.017287  loss: 0.001535  eta: 0h1m  tot: 0h3m13s  (64.9%)25.0%  lr: 0.017247  loss: 0.001538  eta: 0h1m  tot: 0h3m14s  (65.0%)25.1%  lr: 0.017237  loss: 0.001535  eta: 0h1m  tot: 0h3m14s  (65.0%)26.4%  lr: 0.017057  loss: 0.001564  eta: 0h1m  tot: 0h3m15s  (65.3%)26.6%  lr: 0.017037  loss: 0.001564  eta: 0h1m  tot: 0h3m15s  (65.3%)28.2%  lr: 0.016947  loss: 0.001566  eta: 0h1m  tot: 0h3m16s  (65.6%)28.4%  lr: 0.016947  loss: 0.001564  eta: 0h1m  tot: 0h3m16s  (65.7%)29.3%  lr: 0.016817  loss: 0.001564  eta: 0h1m  tot: 0h3m17s  (65.9%)29.6%  lr: 0.016727  loss: 0.001562  eta: 0h1m  tot: 0h3m17s  (65.9%)31.7%  lr: 0.016477  loss: 0.001556  eta: 0h1m  tot: 0h3m18s  (66.3%)32.9%  lr: 0.016386  loss: 0.001568  eta: 0h1m  tot: 0h3m19s  (66.6%)33.4%  lr: 0.016366  loss: 0.001572  eta: 0h1m  tot: 0h3m19s  (66.7%)35.1%  lr: 0.016256  loss: 0.001581  eta: 0h1m  tot: 0h3m20s  (67.0%)%  lr: 0.016196  loss: 0.001575  eta: 0h1m  tot: 0h3m20s  (67.1%)35.8%  lr: 0.016176  loss: 0.001581  eta: 0h1m  tot: 0h3m20s  (67.2%)36.3%  lr: 0.016116  loss: 0.001582  eta: 0h1m  tot: 0h3m21s  (67.3%)36.4%  lr: 0.016096  loss: 0.001583  eta: 0h1m  tot: 0h3m21s  (67.3%)37.2%  lr: 0.015986  loss: 0.001584  eta: 0h1m  tot: 0h3m21s  (67.4%)38.0%  lr: 0.015916  loss: 0.001601  eta: 0h1m  tot: 0h3m22s  (67.6%)%  lr: 0.015846  loss: 0.001602  eta: 0h1m  tot: 0h3m22s  (67.7%)38.7%  lr: 0.015836  loss: 0.001603  eta: 0h1m  tot: 0h3m22s  (67.7%)38.8%  lr: 0.015816  loss: 0.001604  eta: 0h1m  tot: 0h3m22s  (67.8%)39.4%  lr: 0.015736  loss: 0.001598  eta: 0h1m  tot: 0h3m22s  (67.9%)39.6%  lr: 0.015706  loss: 0.001598  eta: 0h1m  tot: 0h3m23s  (67.9%)40.0%  lr: 0.015646  loss: 0.001600  eta: 0h1m  tot: 0h3m23s  (68.0%)40.9%  lr: 0.015556  loss: 0.001603  eta: 0h1m  tot: 0h3m23s  (68.2%)41.0%  lr: 0.015546  loss: 0.001600  eta: 0h1m  tot: 0h3m23s  (68.2%)41.9%  lr: 0.015466  loss: 0.001604  eta: 0h1m  tot: 0h3m24s  (68.4%)42.5%  lr: 0.015425  loss: 0.001593  eta: 0h1m  tot: 0h3m24s  (68.5%)42.6%  lr: 0.015425  loss: 0.001592  eta: 0h1m  tot: 0h3m24s  (68.5%)%  lr: 0.015395  loss: 0.001593  eta: 0h1m  tot: 0h3m25s  (68.6%)43.4%  lr: 0.015385  loss: 0.001587  eta: 0h1m  tot: 0h3m25s  (68.7%)43.5%  lr: 0.015355  loss: 0.001589  eta: 0h1m  tot: 0h3m25s  (68.7%)44.1%  lr: 0.015265  loss: 0.001592  eta: 0h1m  tot: 0h3m25s  (68.8%)44.7%  lr: 0.015195  loss: 0.001590  eta: 0h1m  tot: 0h3m25s  (68.9%)45.1%  lr: 0.015125  loss: 0.001589  eta: 0h1m  tot: 0h3m26s  (69.0%)45.3%  lr: 0.015095  loss: 0.001592  eta: 0h1m  tot: 0h3m26s  (69.1%)45.6%  lr: 0.015095  loss: 0.001589  eta: 0h1m  tot: 0h3m26s  (69.1%)47.0%  lr: 0.014955  loss: 0.001588  eta: 0h1m  tot: 0h3m27s  (69.4%)48.3%  lr: 0.014795  loss: 0.001599  eta: 0h1m  tot: 0h3m27s  (69.7%)48.5%  lr: 0.014765  loss: 0.001603  eta: 0h1m  tot: 0h3m28s  (69.7%)48.8%  lr: 0.014735  loss: 0.001600  eta: 0h1m  tot: 0h3m28s  (69.8%)49.2%  lr: 0.014625  loss: 0.001608  eta: 0h1m  tot: 0h3m28s  (69.8%)49.3%  lr: 0.014615  loss: 0.001607  eta: 0h1m  tot: 0h3m28s  (69.9%)50.1%  lr: 0.014545  loss: 0.001606  eta: 0h1m  tot: 0h3m29s  (70.0%)50.3%  lr: 0.014495  loss: 0.001608  eta: 0h1m  tot: 0h3m29s  (70.1%)51.0%  lr: 0.014425  loss: 0.001607  eta: 0h1m  tot: 0h3m29s  (70.2%)51.5%  lr: 0.014354  loss: 0.001607  eta: 0h1m  tot: 0h3m30s  (70.3%)53.5%  lr: 0.014054  loss: 0.001604  eta: 0h1m  tot: 0h3m31s  (70.7%)53.7%  lr: 0.014054  loss: 0.001605  eta: 0h1m  tot: 0h3m31s  (70.7%)53.7%  lr: 0.014044  loss: 0.001604  eta: 0h1m  tot: 0h3m31s  (70.7%)55.2%  lr: 0.013874  loss: 0.001602  eta: 0h1m  tot: 0h3m32s  (71.0%)55.5%  lr: 0.013864  loss: 0.001599  eta: 0h1m  tot: 0h3m32s  (71.1%)55.6%  lr: 0.013864  loss: 0.001598  eta: 0h1m  tot: 0h3m32s  (71.1%)55.8%  lr: 0.013864  loss: 0.001599  eta: 0h1m  tot: 0h3m32s  (71.2%)56.9%  lr: 0.013784  loss: 0.001604  eta: 0h1m  tot: 0h3m33s  (71.4%)57.0%  lr: 0.013764  loss: 0.001603  eta: 0h1m  tot: 0h3m33s  (71.4%)%  lr: 0.013714  loss: 0.001600  eta: 0h1m  tot: 0h3m33s  (71.5%)58.1%  lr: 0.013634  loss: 0.001599  eta: 0h1m  tot: 0h3m34s  (71.6%)58.8%  lr: 0.013594  loss: 0.001597  eta: 0h1m  tot: 0h3m34s  (71.8%)59.6%  lr: 0.013504  loss: 0.001600  eta: 0h1m  tot: 0h3m35s  (71.9%)60.1%  lr: 0.013393  loss: 0.001603  eta: 0h1m  tot: 0h3m35s  (72.0%)60.9%  lr: 0.013253  loss: 0.001601  eta: 0h1m  tot: 0h3m36s  (72.2%)61.0%  lr: 0.013253  loss: 0.001598  eta: 0h1m  tot: 0h3m36s  (72.2%)%  lr: 0.013203  loss: 0.001597  eta: 0h1m  tot: 0h3m36s  (72.3%)%  lr: 0.013183  loss: 0.001596  eta: 0h1m  tot: 0h3m36s  (72.3%)62.0%  lr: 0.013123  loss: 0.001597  eta: 0h1m  tot: 0h3m36s  (72.4%)62.5%  lr: 0.013103  loss: 0.001595  eta: 0h1m  tot: 0h3m37s  (72.5%)63.0%  lr: 0.013033  loss: 0.001594  eta: 0h1m  tot: 0h3m37s  (72.6%)63.4%  lr: 0.012963  loss: 0.001591  eta: 0h1m  tot: 0h3m37s  (72.7%)63.6%  lr: 0.012953  loss: 0.001591  eta: 0h1m  tot: 0h3m37s  (72.7%)63.6%  lr: 0.012933  loss: 0.001591  eta: 0h1m  tot: 0h3m38s  (72.7%)63.7%  lr: 0.012933  loss: 0.001591  eta: 0h1m  tot: 0h3m38s  (72.7%)63.8%  lr: 0.012933  loss: 0.001594  eta: 0h1m  tot: 0h3m38s  (72.8%)64.6%  lr: 0.012863  loss: 0.001583  eta: 0h1m  tot: 0h3m38s  (72.9%)64.8%  lr: 0.012853  loss: 0.001582  eta: 0h1m  tot: 0h3m38s  (73.0%)65.2%  lr: 0.012793  loss: 0.001582  eta: 0h1m  tot: 0h3m38s  (73.0%)65.3%  lr: 0.012773  loss: 0.001583  eta: 0h1m  tot: 0h3m39s  (73.1%)66.5%  lr: 0.012653  loss: 0.001589  eta: 0h1m  tot: 0h3m39s  (73.3%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.010000  loss: 0.001557  eta: <1min   tot: 0h3m57s  (80.0%)7%  lr: 0.012473  loss: 0.001594  eta: 0h1m  tot: 0h3m40s  (73.5%)68.1%  lr: 0.012393  loss: 0.001592  eta: 0h1m  tot: 0h3m40s  (73.6%)68.3%  lr: 0.012362  loss: 0.001592  eta: 0h1m  tot: 0h3m40s  (73.7%)68.6%  lr: 0.012332  loss: 0.001592  eta: 0h1m  tot: 0h3m41s  (73.7%)69.2%  lr: 0.012292  loss: 0.001594  eta: 0h1m  tot: 0h3m41s  (73.8%)69.4%  lr: 0.012262  loss: 0.001593  eta: 0h1m  tot: 0h3m41s  (73.9%)%  lr: 0.012242  loss: 0.001594  eta: 0h1m  tot: 0h3m41s  (73.9%)69.7%  lr: 0.012222  loss: 0.001592  eta: 0h1m  tot: 0h3m41s  (73.9%)70.0%  lr: 0.012162  loss: 0.001592  eta: 0h1m  tot: 0h3m41s  (74.0%)%  lr: 0.012132  loss: 0.001592  eta: 0h1m  tot: 0h3m42s  (74.0%)71.0%  lr: 0.012062  loss: 0.001597  eta: 0h1m  tot: 0h3m42s  (74.2%)71.1%  lr: 0.012052  loss: 0.001596  eta: 0h1m  tot: 0h3m42s  (74.2%)71.6%  lr: 0.012022  loss: 0.001595  eta: 0h1m  tot: 0h3m42s  (74.3%)71.8%  lr: 0.012012  loss: 0.001593  eta: 0h1m  tot: 0h3m43s  (74.4%)71.9%  lr: 0.012012  loss: 0.001593  eta: 0h1m  tot: 0h3m43s  (74.4%)73.3%  lr: 0.011882  loss: 0.001589  eta: 0h1m  tot: 0h3m44s  (74.7%)%  lr: 0.011862  loss: 0.001588  eta: 0h1m  tot: 0h3m44s  (74.7%)74.1%  lr: 0.011802  loss: 0.001588  eta: 0h1m  tot: 0h3m44s  (74.8%)74.4%  lr: 0.011782  loss: 0.001590  eta: 0h1m  tot: 0h3m44s  (74.9%)74.5%  lr: 0.011772  loss: 0.001591  eta: 0h1m  tot: 0h3m44s  (74.9%)74.6%  lr: 0.011772  loss: 0.001589  eta: 0h1m  tot: 0h3m44s  (74.9%)%  lr: 0.011762  loss: 0.001588  eta: 0h1m  tot: 0h3m44s  (74.9%)74.8%  lr: 0.011762  loss: 0.001587  eta: 0h1m  tot: 0h3m44s  (75.0%)75.3%  lr: 0.011642  loss: 0.001583  eta: 0h1m  tot: 0h3m45s  (75.1%)%  lr: 0.011592  loss: 0.001582  eta: 0h1m  tot: 0h3m45s  (75.1%)76.2%  lr: 0.011552  loss: 0.001579  eta: 0h1m  tot: 0h3m45s  (75.2%)76.3%  lr: 0.011542  loss: 0.001581  eta: 0h1m  tot: 0h3m46s  (75.3%)76.8%  lr: 0.011452  loss: 0.001580  eta: 0h1m  tot: 0h3m46s  (75.4%)77.5%  lr: 0.011361  loss: 0.001579  eta: 0h1m  tot: 0h3m46s  (75.5%)78.0%  lr: 0.011331  loss: 0.001577  eta: 0h1m  tot: 0h3m47s  (75.6%)78.0%  lr: 0.011331  loss: 0.001579  eta: 0h1m  tot: 0h3m47s  (75.6%)78.3%  lr: 0.011291  loss: 0.001578  eta: 0h1m  tot: 0h3m47s  (75.7%)%  lr: 0.011271  loss: 0.001579  eta: 0h1m  tot: 0h3m47s  (75.8%)78.9%  lr: 0.011251  loss: 0.001580  eta: 0h1m  tot: 0h3m47s  (75.8%)80.6%  lr: 0.011051  loss: 0.001579  eta: 0h1m  tot: 0h3m48s  (76.1%)80.9%  lr: 0.010991  loss: 0.001578  eta: 0h1m  tot: 0h3m49s  (76.2%)82.3%  lr: 0.010881  loss: 0.001572  eta: 0h1m  tot: 0h3m49s  (76.5%)82.6%  lr: 0.010871  loss: 0.001571  eta: 0h1m  tot: 0h3m49s  (76.5%)83.1%  lr: 0.010841  loss: 0.001569  eta: 0h1m  tot: 0h3m50s  (76.6%)84.4%  lr: 0.010761  loss: 0.001569  eta: 0h1m  tot: 0h3m50s  (76.9%)84.6%  lr: 0.010721  loss: 0.001567  eta: 0h1m  tot: 0h3m50s  (76.9%)84.9%  lr: 0.010701  loss: 0.001565  eta: 0h1m  tot: 0h3m51s  (77.0%)85.5%  lr: 0.010591  loss: 0.001565  eta: 0h1m  tot: 0h3m51s  (77.1%)86.0%  lr: 0.010561  loss: 0.001565  eta: 0h1m  tot: 0h3m51s  (77.2%)86.6%  lr: 0.010541  loss: 0.001566  eta: 0h1m  tot: 0h3m51s  (77.3%)86.7%  lr: 0.010541  loss: 0.001565  eta: 0h1m  tot: 0h3m51s  (77.3%)%  lr: 0.010531  loss: 0.001563  eta: 0h1m  tot: 0h3m52s  (77.4%)87.2%  lr: 0.010391  loss: 0.001566  eta: 0h1m  tot: 0h3m52s  (77.4%)87.7%  lr: 0.010350  loss: 0.001564  eta: 0h1m  tot: 0h3m52s  (77.5%)87.9%  lr: 0.010330  loss: 0.001563  eta: 0h1m  tot: 0h3m52s  (77.6%)88.5%  lr: 0.010300  loss: 0.001562  eta: 0h1m  tot: 0h3m53s  (77.7%)89.4%  lr: 0.010260  loss: 0.001558  eta: 0h1m  tot: 0h3m53s  (77.9%)90.3%  lr: 0.010200  loss: 0.001552  eta: 0h1m  tot: 0h3m53s  (78.1%)0.010140  loss: 0.001551  eta: 0h1m  tot: 0h3m54s  (78.2%)  eta: 0h1m  tot: 0h3m54s  (78.4%)0.001554  eta: 0h1m  tot: 0h3m54s  (78.6%)0.001554  eta: 0h1m  tot: 0h3m55s  (78.7%)  eta: 0h1m  tot: 0h3m55s  (78.8%)m  tot: 0h3m55s  (79.1%)h3m57s  (79.9%)\n",
      " ---+++                Epoch    3 Train error : 0.00155129 +++--- ☃\n",
      "Training epoch 4: 0.01 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68.9%  lr: 0.002693  loss: 0.001386  eta: <1min   tot: 0h4m38s  (93.8%).8%  lr: 0.009940  loss: 0.001720  eta: <1min   tot: 0h3m57s  (80.2%)1.1%  lr: 0.009890  loss: 0.001546  eta: <1min   tot: 0h3m57s  (80.2%)2.1%  lr: 0.009800  loss: 0.001509  eta: 0h1m  tot: 0h3m58s  (80.4%)3.1%  lr: 0.009700  loss: 0.001388  eta: <1min   tot: 0h3m58s  (80.6%)3.3%  lr: 0.009670  loss: 0.001394  eta: <1min   tot: 0h3m59s  (80.7%)3.7%  lr: 0.009610  loss: 0.001450  eta: <1min   tot: 0h3m59s  (80.7%)4.5%  lr: 0.009530  loss: 0.001441  eta: <1min   tot: 0h4m0s  (80.9%)%  lr: 0.009429  loss: 0.001442  eta: <1min   tot: 0h4m0s  (81.1%)6.3%  lr: 0.009379  loss: 0.001435  eta: <1min   tot: 0h4m1s  (81.3%)%  lr: 0.009379  loss: 0.001443  eta: <1min   tot: 0h4m1s  (81.3%)6.8%  lr: 0.009349  loss: 0.001420  eta: <1min   tot: 0h4m1s  (81.4%)7.3%  lr: 0.009279  loss: 0.001418  eta: <1min   tot: 0h4m1s  (81.5%)7.7%  lr: 0.009259  loss: 0.001399  eta: <1min   tot: 0h4m1s  (81.5%)8.1%  lr: 0.009199  loss: 0.001370  eta: <1min   tot: 0h4m2s  (81.6%)8.7%  lr: 0.009139  loss: 0.001393  eta: <1min   tot: 0h4m2s  (81.7%)8.8%  lr: 0.009129  loss: 0.001386  eta: <1min   tot: 0h4m2s  (81.8%)10.4%  lr: 0.008979  loss: 0.001399  eta: <1min   tot: 0h4m3s  (82.1%)11.0%  lr: 0.008949  loss: 0.001382  eta: <1min   tot: 0h4m3s  (82.2%)%  lr: 0.008849  loss: 0.001366  eta: <1min   tot: 0h4m4s  (82.3%)11.8%  lr: 0.008839  loss: 0.001361  eta: <1min   tot: 0h4m4s  (82.4%)12.5%  lr: 0.008779  loss: 0.001399  eta: <1min   tot: 0h4m4s  (82.5%)12.8%  lr: 0.008719  loss: 0.001401  eta: <1min   tot: 0h4m4s  (82.6%)14.0%  lr: 0.008569  loss: 0.001374  eta: <1min   tot: 0h4m5s  (82.8%)14.6%  lr: 0.008489  loss: 0.001368  eta: <1min   tot: 0h4m5s  (82.9%)15.3%  lr: 0.008428  loss: 0.001354  eta: <1min   tot: 0h4m6s  (83.1%)0h4m6s  (83.1%)15.6%  lr: 0.008388  loss: 0.001367  eta: <1min   tot: 0h4m6s  (83.1%)15.8%  lr: 0.008378  loss: 0.001372  eta: <1min   tot: 0h4m6s  (83.2%)16.1%  lr: 0.008338  loss: 0.001360  eta: <1min   tot: 0h4m6s  (83.2%)16.7%  lr: 0.008248  loss: 0.001352  eta: <1min   tot: 0h4m6s  (83.3%)17.9%  lr: 0.008048  loss: 0.001346  eta: <1min   tot: 0h4m7s  (83.6%)18.0%  lr: 0.008008  loss: 0.001343  eta: <1min   tot: 0h4m7s  (83.6%)18.2%  lr: 0.007968  loss: 0.001340  eta: <1min   tot: 0h4m8s  (83.6%)19.6%  lr: 0.007868  loss: 0.001334  eta: <1min   tot: 0h4m8s  (83.9%)20.3%  lr: 0.007818  loss: 0.001331  eta: <1min   tot: 0h4m9s  (84.1%)20.5%  lr: 0.007778  loss: 0.001331  eta: <1min   tot: 0h4m9s  (84.1%)20.7%  lr: 0.007768  loss: 0.001330  eta: <1min   tot: 0h4m9s  (84.1%)21.6%  lr: 0.007668  loss: 0.001322  eta: <1min   tot: 0h4m10s  (84.3%)22.2%  lr: 0.007638  loss: 0.001317  eta: <1min   tot: 0h4m10s  (84.4%)23.6%  lr: 0.007498  loss: 0.001339  eta: <1min   tot: 0h4m11s  (84.7%)23.8%  lr: 0.007498  loss: 0.001336  eta: <1min   tot: 0h4m11s  (84.8%)24.1%  lr: 0.007457  loss: 0.001342  eta: <1min   tot: 0h4m11s  (84.8%)24.3%  lr: 0.007407  loss: 0.001346  eta: <1min   tot: 0h4m11s  (84.9%)24.5%  lr: 0.007397  loss: 0.001339  eta: <1min   tot: 0h4m11s  (84.9%)24.8%  lr: 0.007337  loss: 0.001336  eta: <1min   tot: 0h4m11s  (85.0%)25.2%  lr: 0.007297  loss: 0.001333  eta: <1min   tot: 0h4m12s  (85.0%)25.3%  lr: 0.007287  loss: 0.001333  eta: <1min   tot: 0h4m12s  (85.1%)26.8%  lr: 0.007147  loss: 0.001338  eta: <1min   tot: 0h4m12s  (85.4%)%  lr: 0.007017  loss: 0.001349  eta: <1min   tot: 0h4m13s  (85.6%)28.2%  lr: 0.006987  loss: 0.001347  eta: <1min   tot: 0h4m13s  (85.6%)29.0%  lr: 0.006867  loss: 0.001355  eta: <1min   tot: 0h4m14s  (85.8%)29.3%  lr: 0.006817  loss: 0.001362  eta: <1min   tot: 0h4m14s  (85.9%)29.5%  lr: 0.006807  loss: 0.001370  eta: <1min   tot: 0h4m14s  (85.9%)29.7%  lr: 0.006787  loss: 0.001370  eta: <1min   tot: 0h4m15s  (85.9%)30.2%  lr: 0.006757  loss: 0.001368  eta: <1min   tot: 0h4m15s  (86.0%)31.0%  lr: 0.006687  loss: 0.001358  eta: <1min   tot: 0h4m15s  (86.2%)32.2%  lr: 0.006617  loss: 0.001369  eta: <1min   tot: 0h4m16s  (86.4%)33.1%  lr: 0.006527  loss: 0.001363  eta: <1min   tot: 0h4m16s  (86.6%)33.6%  lr: 0.006497  loss: 0.001367  eta: <1min   tot: 0h4m17s  (86.7%)35.4%  lr: 0.006296  loss: 0.001368  eta: <1min   tot: 0h4m17s  (87.1%)%  lr: 0.006186  loss: 0.001376  eta: <1min   tot: 0h4m18s  (87.3%)36.5%  lr: 0.006176  loss: 0.001377  eta: <1min   tot: 0h4m18s  (87.3%)36.9%  lr: 0.006126  loss: 0.001376  eta: <1min   tot: 0h4m18s  (87.4%)38.0%  lr: 0.005956  loss: 0.001373  eta: <1min   tot: 0h4m19s  (87.6%)38.3%  lr: 0.005906  loss: 0.001374  eta: <1min   tot: 0h4m19s  (87.7%)38.6%  lr: 0.005906  loss: 0.001369  eta: <1min   tot: 0h4m20s  (87.7%)38.7%  lr: 0.005886  loss: 0.001369  eta: <1min   tot: 0h4m20s  (87.7%)38.8%  lr: 0.005866  loss: 0.001368  eta: <1min   tot: 0h4m20s  (87.8%)39.8%  lr: 0.005726  loss: 0.001366  eta: <1min   tot: 0h4m20s  (88.0%)41.1%  lr: 0.005646  loss: 0.001361  eta: <1min   tot: 0h4m21s  (88.2%)42.3%  lr: 0.005566  loss: 0.001354  eta: <1min   tot: 0h4m21s  (88.5%)43.7%  lr: 0.005476  loss: 0.001347  eta: <1min   tot: 0h4m22s  (88.7%)%  lr: 0.005446  loss: 0.001346  eta: <1min   tot: 0h4m22s  (88.8%)44.6%  lr: 0.005365  loss: 0.001344  eta: <1min   tot: 0h4m22s  (88.9%)45.1%  lr: 0.005335  loss: 0.001340  eta: <1min   tot: 0h4m23s  (89.0%)45.8%  lr: 0.005285  loss: 0.001337  eta: <1min   tot: 0h4m23s  (89.2%)46.1%  lr: 0.005285  loss: 0.001337  eta: <1min   tot: 0h4m23s  (89.2%)46.2%  lr: 0.005275  loss: 0.001339  eta: <1min   tot: 0h4m23s  (89.2%)%  lr: 0.005155  loss: 0.001336  eta: <1min   tot: 0h4m24s  (89.4%)47.5%  lr: 0.005135  loss: 0.001334  eta: <1min   tot: 0h4m24s  (89.5%)48.4%  lr: 0.005045  loss: 0.001337  eta: <1min   tot: 0h4m25s  (89.7%)49.1%  lr: 0.004955  loss: 0.001347  eta: <1min   tot: 0h4m25s  (89.8%)%  lr: 0.004935  loss: 0.001347  eta: <1min   tot: 0h4m25s  (89.8%)49.7%  lr: 0.004865  loss: 0.001346  eta: <1min   tot: 0h4m26s  (89.9%)50.8%  lr: 0.004775  loss: 0.001346  eta: <1min   tot: 0h4m26s  (90.2%)51.1%  lr: 0.004705  loss: 0.001348  eta: <1min   tot: 0h4m27s  (90.2%)51.5%  lr: 0.004605  loss: 0.001345  eta: <1min   tot: 0h4m27s  (90.3%)51.7%  lr: 0.004605  loss: 0.001345  eta: <1min   tot: 0h4m27s  (90.3%)53.5%  lr: 0.004435  loss: 0.001355  eta: <1min   tot: 0h4m28s  (90.7%)55.2%  lr: 0.004264  loss: 0.001356  eta: <1min   tot: 0h4m29s  (91.0%)55.8%  lr: 0.004194  loss: 0.001359  eta: <1min   tot: 0h4m29s  (91.2%)55.9%  lr: 0.004164  loss: 0.001357  eta: <1min   tot: 0h4m29s  (91.2%)%  lr: 0.004144  loss: 0.001358  eta: <1min   tot: 0h4m29s  (91.2%)56.1%  lr: 0.004144  loss: 0.001357  eta: <1min   tot: 0h4m30s  (91.2%)57.2%  lr: 0.004004  loss: 0.001363  eta: <1min   tot: 0h4m30s  (91.4%)57.5%  lr: 0.003974  loss: 0.001367  eta: <1min   tot: 0h4m30s  (91.5%)57.6%  lr: 0.003964  loss: 0.001365  eta: <1min   tot: 0h4m31s  (91.5%)57.8%  lr: 0.003944  loss: 0.001365  eta: <1min   tot: 0h4m31s  (91.6%)58.7%  lr: 0.003824  loss: 0.001365  eta: <1min   tot: 0h4m31s  (91.7%)%  lr: 0.003804  loss: 0.001370  eta: <1min   tot: 0h4m32s  (91.8%)60.0%  lr: 0.003634  loss: 0.001365  eta: <1min   tot: 0h4m32s  (92.0%)60.2%  lr: 0.003614  loss: 0.001364  eta: <1min   tot: 0h4m33s  (92.0%)60.3%  lr: 0.003604  loss: 0.001364  eta: <1min   tot: 0h4m33s  (92.1%)60.5%  lr: 0.003554  loss: 0.001361  eta: <1min   tot: 0h4m33s  (92.1%)60.9%  lr: 0.003484  loss: 0.001364  eta: <1min   tot: 0h4m33s  (92.2%)61.8%  lr: 0.003333  loss: 0.001365  eta: <1min   tot: 0h4m34s  (92.4%)61.9%  lr: 0.003303  loss: 0.001367  eta: <1min   tot: 0h4m34s  (92.4%)62.4%  lr: 0.003273  loss: 0.001367  eta: <1min   tot: 0h4m34s  (92.5%)62.7%  lr: 0.003253  loss: 0.001364  eta: <1min   tot: 0h4m34s  (92.5%)63.5%  lr: 0.003193  loss: 0.001365  eta: <1min   tot: 0h4m35s  (92.7%)65.1%  lr: 0.003013  loss: 0.001371  eta: <1min   tot: 0h4m36s  (93.0%)65.8%  lr: 0.003013  loss: 0.001374  eta: <1min   tot: 0h4m36s  (93.2%)%  lr: 0.002883  loss: 0.001379  eta: <1min   tot: 0h4m37s  (93.3%)%  lr: 0.002803  loss: 0.001383  eta: <1min   tot: 0h4m38s  (93.6%)68.6%  lr: 0.002733  loss: 0.001385  eta: <1min   tot: 0h4m38s  (93.7%)69.0%  lr: 0.002683  loss: 0.001386  eta: <1min   tot: 0h4m38s  (93.8%)69.0%  lr: 0.002653  loss: 0.001386  eta: <1min   tot: 0h4m38s  (93.8%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.000000  loss: 0.001372  eta: <1min   tot: 0h4m55s  (100.0%).6%  lr: 0.002633  loss: 0.001389  eta: <1min   tot: 0h4m38s  (93.9%)70.1%  lr: 0.002613  loss: 0.001387  eta: <1min   tot: 0h4m39s  (94.0%)70.5%  lr: 0.002603  loss: 0.001387  eta: <1min   tot: 0h4m39s  (94.1%)71.0%  lr: 0.002573  loss: 0.001392  eta: <1min   tot: 0h4m39s  (94.2%)72.2%  lr: 0.002402  loss: 0.001395  eta: <1min   tot: 0h4m40s  (94.4%)72.5%  lr: 0.002372  loss: 0.001395  eta: <1min   tot: 0h4m40s  (94.5%)72.8%  lr: 0.002352  loss: 0.001393  eta: <1min   tot: 0h4m40s  (94.6%)72.9%  lr: 0.002352  loss: 0.001394  eta: <1min   tot: 0h4m40s  (94.6%)73.3%  lr: 0.002282  loss: 0.001394  eta: <1min   tot: 0h4m40s  (94.7%)%  lr: 0.002232  loss: 0.001391  eta: <1min   tot: 0h4m41s  (94.7%)74.3%  lr: 0.002172  loss: 0.001390  eta: <1min   tot: 0h4m41s  (94.9%)74.8%  lr: 0.002162  loss: 0.001389  eta: <1min   tot: 0h4m41s  (95.0%)75.2%  lr: 0.002142  loss: 0.001387  eta: <1min   tot: 0h4m41s  (95.0%)76.9%  lr: 0.001942  loss: 0.001388  eta: <1min   tot: 0h4m42s  (95.4%)77.2%  lr: 0.001882  loss: 0.001390  eta: <1min   tot: 0h4m43s  (95.4%)77.5%  lr: 0.001862  loss: 0.001388  eta: <1min   tot: 0h4m43s  (95.5%)77.7%  lr: 0.001832  loss: 0.001387  eta: <1min   tot: 0h4m43s  (95.5%)%  lr: 0.001752  loss: 0.001386  eta: <1min   tot: 0h4m43s  (95.7%)78.6%  lr: 0.001642  loss: 0.001385  eta: <1min   tot: 0h4m44s  (95.7%)78.9%  lr: 0.001632  loss: 0.001384  eta: <1min   tot: 0h4m44s  (95.8%)%  lr: 0.001622  loss: 0.001383  eta: <1min   tot: 0h4m44s  (96.0%)80.1%  lr: 0.001562  loss: 0.001382  eta: <1min   tot: 0h4m44s  (96.0%)80.9%  lr: 0.001482  loss: 0.001382  eta: <1min   tot: 0h4m45s  (96.2%)81.9%  lr: 0.001401  loss: 0.001379  eta: <1min   tot: 0h4m46s  (96.4%)82.1%  lr: 0.001361  loss: 0.001381  eta: <1min   tot: 0h4m46s  (96.4%)82.2%  lr: 0.001341  loss: 0.001381  eta: <1min   tot: 0h4m46s  (96.4%)82.7%  lr: 0.001311  loss: 0.001379  eta: <1min   tot: 0h4m46s  (96.5%)82.8%  lr: 0.001291  loss: 0.001379  eta: <1min   tot: 0h4m46s  (96.6%)83.4%  lr: 0.001281  loss: 0.001379  eta: <1min   tot: 0h4m46s  (96.7%)84.0%  lr: 0.001191  loss: 0.001380  eta: <1min   tot: 0h4m47s  (96.8%)%  lr: 0.001101  loss: 0.001379  eta: <1min   tot: 0h4m47s  (96.9%)84.6%  lr: 0.001051  loss: 0.001379  eta: <1min   tot: 0h4m47s  (96.9%)85.6%  lr: 0.000951  loss: 0.001385  eta: <1min   tot: 0h4m47s  (97.1%)85.8%  lr: 0.000901  loss: 0.001386  eta: <1min   tot: 0h4m48s  (97.2%)85.9%  lr: 0.000881  loss: 0.001386  eta: <1min   tot: 0h4m48s  (97.2%)86.8%  lr: 0.000761  loss: 0.001385  eta: <1min   tot: 0h4m48s  (97.4%)86.9%  lr: 0.000761  loss: 0.001384  eta: <1min   tot: 0h4m48s  (97.4%)87.0%  lr: 0.000751  loss: 0.001384  eta: <1min   tot: 0h4m48s  (97.4%)%  lr: 0.000731  loss: 0.001384  eta: <1min   tot: 0h4m49s  (97.5%)88.0%  lr: 0.000711  loss: 0.001386  eta: <1min   tot: 0h4m49s  (97.6%)88.6%  lr: 0.000651  loss: 0.001384  eta: <1min   tot: 0h4m49s  (97.7%)88.7%  lr: 0.000641  loss: 0.001384  eta: <1min   tot: 0h4m49s  (97.7%)88.8%  lr: 0.000641  loss: 0.001384  eta: <1min   tot: 0h4m49s  (97.8%)89.2%  lr: 0.000581  loss: 0.001383  eta: <1min   tot: 0h4m50s  (97.8%)89.7%  lr: 0.000541  loss: 0.001380  eta: <1min   tot: 0h4m50s  (97.9%)89.8%  lr: 0.000541  loss: 0.001380  eta: <1min   tot: 0h4m50s  (98.0%)90.8%  lr: 0.000491  loss: 0.001380  eta: <1min   tot: 0h4m51s  (98.2%)%  lr: 0.000431  loss: 0.001381  eta: <1min   tot: 0h4m51s  (98.4%)92.5%  lr: 0.000380  loss: 0.001381  eta: <1min   tot: 0h4m51s  (98.5%)92.7%  lr: 0.000350  loss: 0.001379  eta: <1min   tot: 0h4m52s  (98.5%)93.3%  lr: 0.000300  loss: 0.001376  eta: <1min   tot: 0h4m52s  (98.7%)93.7%  lr: 0.000260  loss: 0.001377  eta: <1min   tot: 0h4m52s  (98.7%)94.4%  lr: 0.000220  loss: 0.001374  eta: <1min   tot: 0h4m53s  (98.9%)94.7%  lr: 0.000220  loss: 0.001374  eta: <1min   tot: 0h4m53s  (98.9%)96.3%  lr: 0.000050  loss: 0.001371  eta: <1min   tot: 0h4m53s  (99.3%)0h4m54s  (99.4%)4m54s  (99.6%)4m54s  (99.8%)\n",
      " ---+++                Epoch    4 Train error : 0.00136186 +++--- ☃\n",
      "Saving model to file : starspace_embedding\n",
      "Saving model in tsv format : starspace_embedding.tsv\n"
     ]
    }
   ],
   "source": [
    "! ./Starspace/starspace train -trainFile data/train_prepared.tsv -model starspace_embedding -trainMode 3 -minCount 2 -verbose true -fileFormat labelDoc -negSearchLimit 10 -lr 0.05\n",
    "######### TRAINING HAPPENING HERE #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "starspace_embeddings = {}\n",
    "for line in open('starspace_embedding.tsv', encoding='utf-8'):\n",
    "    tokens = line.strip().split('\\t')\n",
    "    starspace_embeddings[tokens[0]] = np.array(tokens[1:],dtype=np.float64)\n",
    "    ######### YOUR CODE HERE #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:33<00:00, 65.02it/s]\n"
     ]
    }
   ],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in tqdm(prepared_validation):\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.520 | Hits@   1: 0.520\n",
      "DCG@   5: 0.616 | Hits@   5: 0.699\n",
      "DCG@  10: 0.635 | Hits@  10: 0.757\n",
      "DCG@ 100: 0.666 | Hits@ 100: 0.906\n",
      "DCG@ 500: 0.675 | Hits@ 500: 0.981\n",
      "DCG@1000: 0.678 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task StarSpaceRanks is: 75\t48\t96\t20\t15\t78\t71\t89\t37\t57\t29\t49\t10\t6\t99\t67\t9\t74\t73\t94\t65\t28\t27\t83\t95\t30\t66\t98\t58\t4\t64\t36\t62\t69\t2...\n"
     ]
    }
   ],
   "source": [
    "starspace_ranks_results = []\n",
    "prepared_test_data = 'data/test_prepared.tsv'######### YOUR CODE HERE #############\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these parts:\n",
      "Task Question2Vec: 0.01929389126598835\n",
      "-0.02872721292078495\n",
      "0.0460561104118824\n",
      "0.0852593332529068\n",
      "0.0243055559694767\n",
      "-0...\n",
      "Task HitsCount: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.30000000000000004\n",
      "0.4\n",
      "0.5\n",
      "0....\n",
      "Task DCGScore: 1.0\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.3333333333333333\n",
      "0.5436432511904858\n",
      "0.7103099178...\n",
      "Task W2VTokenizedRanks: 95\t94\t7\t9\t64\t37\t32\t93\t24\t100\t98\t17\t60\t6\t97\t49\t70\t38\t42\t96\t30\t21\t2\t65\t67\t45\t27\t26\t57\t62\t11\t88\t56\t66\t7...\n",
      "Task StarSpaceRanks: 75\t48\t96\t20\t15\t78\t71\t89\t37\t57\t29\t49\t10\t6\t99\t67\t9\t74\t73\t94\t65\t28\t27\t83\t95\t30\t66\t98\t58\t4\t64\t36\t62\t69\t2...\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = 'tianqi.terence.liu@gmail.com'# EMAIL \n",
    "STUDENT_TOKEN = 'ZVYXt5j9EFxAH3b3'# TOKEN \n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
